[
  {
    "objectID": "posts/Tie-Ballast Detection/index.html",
    "href": "posts/Tie-Ballast Detection/index.html",
    "title": "Tie-Ballast Identification",
    "section": "",
    "text": "This study develops an effective system for in-motion and autonomous identification of crossties and ballast, toward implementing non-contact sensors that can evaluate the condition of railroad structures. An array of sensors that include distance LIDAR sensors, magnetic sensors, and cameras are employed to test a prototype system in motion, onboard a remotely controlled track cart that can travel at speeds of up to 10 mph. The tests are performed on revenue-service tracks and the measurements are used to develop autonomous post-processing approaches that can be readily adopted by the railroads. Two distinct techniques for the LIDAR sensors are explored. Next, a machine learning model is developed to achieve the task with potentially more accuracy. To this end, three machine learning models, using three types of inputs, are developed to identify the optimal model. The DecisionTree algorithm coupled with the standard deviation of the difference between two distance sensors proved to be the most effective.\nfrom google.colab import drive\ndrive.mount('/content/drive')\nDefine a custom function for plotting\ndef interactive_plot_scatter(df, x_axis, y_axis):\n  import plotly.express as px\n\n  fig = px.scatter(df, x_axis, y_axis)\n  return fig.show()\n\ndef interactive_plot_line(df, x_axis, y_axis):\n  import plotly.express as px\n\n  fig = px.line(df, x_axis, y_axis)\n  return fig.show()\n\ndef tach_cleaning(df):\n  tach = np.zeros(df['Tach'].shape[0])\n  for i in range(1, df['Tach'].shape[0]):\n    if df.iloc[i]['Tach'] &gt; 2.2:\n      tach[i] = 5\n  df['Tach_p'] = tach\n  return df\n\ndef position_string(df: dict) -&gt; str:\n  df['Pos'] = 0.000000\n  pos = df['Pos']\n  c = 0;\n  delta = float((1/36) * 7.25 * np.pi / 12);\n  for i in range(1, pos.shape[0]):\n    if (np.abs(df.iloc[i]['Tach']-df.iloc[i-1]['Tach'])&gt;1):\n      c = c + 1\n    pos[i] = delta * c\n  return pos\n\ndef remove_Keyence_dropout(df):\n  df = df[df['left_disp']&lt;1.15]\n  df = df[df['right_disp']&lt;0.925]\n  return df\n\ndef remove_outliers(df):\n  Q1 = np.percentile(df['right_disp'], 25,\n                   interpolation = 'midpoint')\n\n  Q3 = np.percentile(df['right_disp'], 75,\n                   interpolation = 'midpoint')\n  IQR = Q3 - Q1\n\n  up = Q3+1.5*IQR\n  low = Q1-1.5*IQR\n\n  df = df[df['right_disp']&lt;up]\n  df = df[df['right_disp']&gt;low]\n\n  Q1 = np.percentile(df['left_disp'], 25,\n                   interpolation = 'midpoint')\n\n  Q3 = np.percentile(df['left_disp'], 75,\n                   interpolation = 'midpoint')\n  IQR = Q3 - Q1\n\n  up = Q3+1.5*IQR\n  low = Q1-1.5*IQR\n\n  df = df[df['left_disp']&lt;up]\n  df = df[df['left_disp']&gt;low]\n  return df\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef custom_multiplot(x, y_sets, title=None, xlabel=None, ylabel=None, legend_labels=None, legend_loc='best', grid=True, save_as=None, colors=None, title_size=16, label_font_size=12, tick_font_size=10, figsize=(10, 6), xlim=None, ylim=None, xlog=False, ylog=False, scatter=False):\n    \"\"\"\n    Create a customized plot with multiple y-axis parameters using Matplotlib.\n\n    Parameters:\n    - x: x-axis data (list or NumPy array)\n    - y_sets: List of y-axis data sets (list of lists or NumPy arrays)\n    - title: Plot title (string, optional)\n    - xlabel: Label for the x-axis (string, optional)\n    - ylabel: Label for the y-axis (string, optional)\n    - legend_labels: Labels for the legend (list of strings, optional)\n    - legend_loc: Location of the legend ('best', 'upper left', 'upper right', 'lower left', 'lower right', etc.)\n    - grid: Display grid lines (boolean, optional)\n    - save_as: File name to save the plot as an image (string, optional)\n    - colors: List of line colors (list of strings or tuples, optional)\n    - title_size: Font size for the plot title (int, optional)\n    - label_font_size: Font size for axis labels and legend (int, optional)\n    - tick_font_size: Font size for tick labels (int, optional)\n    - figsize: Figure size as a tuple (width, height) (optional)\n    - xlim: Tuple specifying the x-axis limits (e.g., (xmin, xmax)) (optional)\n    - ylim: Tuple specifying the y-axis limits (e.g., (ymin, ymax)) (optional)\n    - xlog: Enable logarithmic scaling for the x-axis (boolean, optional)\n    - ylog: Enable logarithmic scaling for the y-axis (boolean, optional)\n\n    Returns:\n    - None\n    \"\"\"\n    plt.figure(figsize=figsize)  # Adjust the figure size\n\n    if colors is None:\n        colors = ['blue', 'red', 'green', 'purple', 'orange', 'cyan', 'magenta']\n\n    for i, y in enumerate(y_sets):\n        color = colors[i % len(colors)]\n        label = legend_labels[i] if legend_labels and i &lt; len(legend_labels) else None\n\n        if scatter:\n            plt.scatter(x, y, label=label, color=color, s=30)\n        elif xlog:\n            plt.semilogx(x, y, label=label, color=color, linewidth=2)\n        elif ylog:\n            plt.semilogy(x, y, label=label, color=color, linewidth=2)\n        else:\n            plt.plot(x, y, label=label, color=color, linewidth=2)\n\n    if legend_labels:\n        plt.legend(legend_labels, loc=legend_loc, fontsize=label_font_size)\n\n    if title:\n        plt.title(title, fontsize=title_size)\n\n    if xlabel:\n        plt.xlabel(xlabel, fontsize=label_font_size)\n\n    if ylabel:\n        plt.ylabel(ylabel, fontsize=label_font_size)\n\n    if grid:\n        plt.grid(True)\n\n    if xlim:\n        plt.xlim(xlim)\n\n    if ylim:\n        plt.ylim(ylim)\n\n    plt.xticks(fontsize=tick_font_size)\n    plt.yticks(fontsize=tick_font_size)\n\n    if xlog:\n        plt.xscale('log')\n    if ylog:\n        plt.yscale('log')\n    ax = plt.gca()\n    #ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    #ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n    if save_as:\n        plt.savefig(save_as, dpi=300, bbox_inches='tight')\n\n    plt.show()\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support as score\nImport the required libraries\ndf = pd.read_csv('/content/drive/MyDrive/CVeSS/TB/Huckelberry/Feb_8/DE9C0013.csv')\n\ndf.set_axis(['sec', 'left_disp', 'right_disp', 'Yaw_disp', 'mag', 'x', 'y', 'z', 'temp', 'humid', 'sync', 'Tach', 'bat', 'date', 'UTC'], axis=\"columns\", inplace=True)\nThe provided dataset includes measured data, with a focus on the “sec,” “left_disp,” and “right_disp” columns, representing time, left sensor measurements, and right sensor measurements, respectively. The left and right sensors gauge the distance from the ground surface to a constant reference point, essentially capturing the surface figure of the track.\ncustom_multiplot(\n    df['sec'], [df['right_disp'], df['left_disp']],\n    title='Raw data',\n    xlabel='Time (s)',\n    ylabel='Distance',\n    legend_labels=['rigth', 'left'],\n    colors=['red', 'green'],\n    title_size=20,\n    label_font_size=14,\n    tick_font_size=12,\n    grid=True,\n    figsize=(20, 6),\n    xlog=False,\n    ylog=False\n)\n\nA funtion is defined to remove the outliers using IQR method\ndf = remove_outliers(df)\ndf.reset_index(inplace=True,drop=True)\ncustom_multiplot(\n    df['sec'], [df['right_disp'], df['left_disp']],\n    title='Filtered data',\n    xlabel='Time (s)',\n    ylabel='Distance',\n    legend_labels=['rigth', 'left'],\n    colors=['red', 'green'],\n    title_size=20,\n    label_font_size=14,\n    tick_font_size=12,\n    grid=True,\n    figsize=(20, 6),\n    xlog=False,\n    ylog=False\n)\n\nA small section of the data (from time 100 s to 150 s) is selected to be used for training\ndf_train = df[(df['sec']&lt;150) & (df['sec']&gt;100)]\n\ncustom_multiplot(\n    df_train['sec'], [df_train['right_disp'], df_train['left_disp']],\n    title='Raw data',\n    xlabel='Time (s)',\n    ylabel='Distance',\n    legend_labels=['rigth', 'left'],\n    colors=['red', 'green'],\n    title_size=20,\n    label_font_size=14,\n    tick_font_size=12,\n    grid=True,\n    figsize=(20, 6),\n    xlog=False,\n    ylog=False\n)\n\nBefore starting the training, we should manually label the data points related to ties and ballast. In the training dataframe, there are 55 ties. For each tie, start and end times are recorded in a dataframe called tb. We eill use this information to label all the data points.\ntb = pd.read_excel('/content/drive/MyDrive/CVeSS/TB/Huckelberry/Feb_8/tie_ballast.xlsx')\nLabeling all the data points take place here. In this block, we go through each data point and we check to see if it belongs to tb dataframe. If it was we have Tie (1), else, ballast (0).\nlabels = np.zeros(len(df_train))\n\nfor i in range(len(df_train)):\n  for j in range(len(tb['start'])):\n    if df_train.iloc[i]['sec']&lt;tb.iloc[j]['end']:\n      if df_train.iloc[i]['sec']&gt;tb.iloc[j]['start']:\n        labels[i] = 1\n\n\ndf_train['labels'] = labels\n\ncustom_multiplot(\n    df_train['sec'], [df_train['right_disp'], df_train['left_disp'], df_train['labels']],\n    title='Raw data',\n    xlabel='Time (s)',\n    ylabel='Distance',\n    legend_labels=['rigth', 'left', 'tie/ballast'],\n    colors=['red', 'green', 'blue'],\n    title_size=20,\n    label_font_size=14,\n    tick_font_size=12,\n    grid=True,\n    figsize=(20, 6),\n    xlog=False,\n    ylog=False\n)\n\nSave and reload the data for futur use\ndf_train.to_csv('/content/drive/MyDrive/CVeSS/TB/Huckelberry/Feb_8/Labeled_DE9C0013.csv')\ndf_train = pd.read_csv('/content/drive/MyDrive/CVeSS/TB/Huckelberry/Feb_8/Labeled_DE9C0013.csv')\n\n\nAll the measured data will be stored in sequences of certain size. So the machine learning model will look at sequence of data not a single data point.\n\n\n\nThree inputs are defined here to investigate which one yields better results.\n\nRaw measurment\nDifference of right and left\nStandard deviation of difference\n\nws = 50\n\nRDbatch = []\nLDbatch = []\nDiffbatch = []\nSTDDiff = []\ntarget = []\n\n#len(df_train) // ws * ws\nfor i in range(ws, len(df_train)):\n  RDbatch.append(df_train.iloc[i-ws:i]['right_disp'])\n  LDbatch.append(df_train.iloc[i-ws:i]['left_disp'])\n  Diffbatch.append(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp'])\n  STDDiff.append(np.std(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp']))\n  if sum(df_train.iloc[i-ws:i]['labels']) &gt;= ws/2:\n    target.append(1)\n  else:\n    target.append(0)\nSTDofDiff = np.zeros([len(STDDiff), ws])\n\nfor i in range(ws, len(STDofDiff)):\n  STDofDiff[i-ws][:] = np.array([STDDiff[i-ws:i]])\nData is scaled using StandardScaler\n\n\nX_train, X_test, y_train, y_test = train_test_split(np.array(LDbatch), target, test_size=0.25, random_state=0)\n\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)\n\ndecision_tree = DecisionTreeClassifier(random_state=456)\ndecision_tree.fit(X_train, y_train)\n\ny_pred = decision_tree.predict(X_test)\nprecision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\nprint(\"F Score for using one measurement : \", fscore)\nF Score for using one measurement : 0.959993349532803\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(np.array(Diffbatch), target, test_size=0.25, random_state=0)\n\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)\n\ndecision_tree = DecisionTreeClassifier(random_state=456)\ndecision_tree.fit(X_train, y_train)\n\ny_pred = decision_tree.predict(X_test)\nprecision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\nprint(\"F Score for using difference of measurments : \", fscore)\nF Score for using difference of measurments : 0.9173980177749999\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(np.array(STDofDiff), target, test_size=0.25, random_state=0)\n\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)\n\ndecision_tree = DecisionTreeClassifier(random_state=456)\ndecision_tree.fit(X_train, y_train)\n\ny_pred = decision_tree.predict(X_test)\nprecision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\nprint(\"F Score for using standard deviation of difference of measurmentst : \", fscore)\nF Score for using standard deviation of difference of measurmentst : 0.9399797522284663\nIt can be seen that the raw measurments resulted in a better F-score. However, STD of difference more robust and is less sensitive in tie elevation. So we decided to move on with STD of difference as input.\n\n\n\n\nIn this section the optimum window size will be selected\nt = []\nfscoreVal = []\nmodel = []\n\nfor w in range(50, 251, 50):\n  ws = w\n  RDbatch = []\n  LDbatch = []\n  Diffbatch = []\n  STDDiff = []\n  target = []\n\n  #len(df_train) // ws * ws\n  for i in range(ws, len(df_train)):\n    RDbatch.append(df_train.iloc[i-ws:i]['right_disp'])\n    LDbatch.append(df_train.iloc[i-ws:i]['left_disp'])\n    Diffbatch.append(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp'])\n    STDDiff.append(np.std(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp']))\n    if sum(df_train.iloc[i-ws:i]['labels']) &gt;= ws*0.75:\n      target.append(1)\n    else:\n      target.append(0)\n\n  STDofDiff = np.zeros([len(STDDiff), ws])\n  for i in range(ws, len(STDofDiff)):\n    STDofDiff[i-ws][:] = np.array([STDDiff[i-ws:i]])\n\n\n  X_train, X_test, y_train, y_test = train_test_split(STDofDiff, target, test_size=0.25, random_state=0)\n  scaler = StandardScaler()\n  X_train=scaler.fit_transform(X_train)\n  X_test=scaler.fit_transform(X_test)\n  decision_tree = DecisionTreeClassifier(random_state=456)\n  decision_tree.fit(X_train, y_train)\n  y_pred = decision_tree.predict(X_test)\n  precision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\n  model.append(decision_tree)\n  t.append(ws)\n  fscoreVal.append(fscore)\ncustom_multiplot(\n    t, [fscoreVal],\n    title='Logarithmic Scale Plot',\n    xlabel='X-axis (log scale)',\n    ylabel='Y-axis (log scale)',\n    legend_labels=['y1', 'y2'],\n    colors=['blue', 'red'],\n    title_size=20,\n    label_font_size=14,\n    tick_font_size=12,\n    grid=True,\n    save_as='logarithmic_plot.png',\n    figsize=(10, 6),\n    xlog=False,\n    ylog=False\n)"
  },
  {
    "objectID": "posts/Tie-Ballast Detection/index.html#segmentizing-the-data",
    "href": "posts/Tie-Ballast Detection/index.html#segmentizing-the-data",
    "title": "Tie-Ballast Identification",
    "section": "",
    "text": "All the measured data will be stored in sequences of certain size. So the machine learning model will look at sequence of data not a single data point."
  },
  {
    "objectID": "posts/Tie-Ballast Detection/index.html#inputs",
    "href": "posts/Tie-Ballast Detection/index.html#inputs",
    "title": "Tie-Ballast Identification",
    "section": "",
    "text": "Three inputs are defined here to investigate which one yields better results.\n\nRaw measurment\nDifference of right and left\nStandard deviation of difference\n\nws = 50\n\nRDbatch = []\nLDbatch = []\nDiffbatch = []\nSTDDiff = []\ntarget = []\n\n#len(df_train) // ws * ws\nfor i in range(ws, len(df_train)):\n  RDbatch.append(df_train.iloc[i-ws:i]['right_disp'])\n  LDbatch.append(df_train.iloc[i-ws:i]['left_disp'])\n  Diffbatch.append(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp'])\n  STDDiff.append(np.std(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp']))\n  if sum(df_train.iloc[i-ws:i]['labels']) &gt;= ws/2:\n    target.append(1)\n  else:\n    target.append(0)\nSTDofDiff = np.zeros([len(STDDiff), ws])\n\nfor i in range(ws, len(STDofDiff)):\n  STDofDiff[i-ws][:] = np.array([STDDiff[i-ws:i]])\nData is scaled using StandardScaler\n\n\nX_train, X_test, y_train, y_test = train_test_split(np.array(LDbatch), target, test_size=0.25, random_state=0)\n\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)\n\ndecision_tree = DecisionTreeClassifier(random_state=456)\ndecision_tree.fit(X_train, y_train)\n\ny_pred = decision_tree.predict(X_test)\nprecision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\nprint(\"F Score for using one measurement : \", fscore)\nF Score for using one measurement : 0.959993349532803\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(np.array(Diffbatch), target, test_size=0.25, random_state=0)\n\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)\n\ndecision_tree = DecisionTreeClassifier(random_state=456)\ndecision_tree.fit(X_train, y_train)\n\ny_pred = decision_tree.predict(X_test)\nprecision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\nprint(\"F Score for using difference of measurments : \", fscore)\nF Score for using difference of measurments : 0.9173980177749999\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(np.array(STDofDiff), target, test_size=0.25, random_state=0)\n\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)\n\ndecision_tree = DecisionTreeClassifier(random_state=456)\ndecision_tree.fit(X_train, y_train)\n\ny_pred = decision_tree.predict(X_test)\nprecision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\nprint(\"F Score for using standard deviation of difference of measurmentst : \", fscore)\nF Score for using standard deviation of difference of measurmentst : 0.9399797522284663\nIt can be seen that the raw measurments resulted in a better F-score. However, STD of difference more robust and is less sensitive in tie elevation. So we decided to move on with STD of difference as input."
  },
  {
    "objectID": "posts/Tie-Ballast Detection/index.html#optimum-window-size",
    "href": "posts/Tie-Ballast Detection/index.html#optimum-window-size",
    "title": "Tie-Ballast Identification",
    "section": "",
    "text": "In this section the optimum window size will be selected\nt = []\nfscoreVal = []\nmodel = []\n\nfor w in range(50, 251, 50):\n  ws = w\n  RDbatch = []\n  LDbatch = []\n  Diffbatch = []\n  STDDiff = []\n  target = []\n\n  #len(df_train) // ws * ws\n  for i in range(ws, len(df_train)):\n    RDbatch.append(df_train.iloc[i-ws:i]['right_disp'])\n    LDbatch.append(df_train.iloc[i-ws:i]['left_disp'])\n    Diffbatch.append(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp'])\n    STDDiff.append(np.std(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp']))\n    if sum(df_train.iloc[i-ws:i]['labels']) &gt;= ws*0.75:\n      target.append(1)\n    else:\n      target.append(0)\n\n  STDofDiff = np.zeros([len(STDDiff), ws])\n  for i in range(ws, len(STDofDiff)):\n    STDofDiff[i-ws][:] = np.array([STDDiff[i-ws:i]])\n\n\n  X_train, X_test, y_train, y_test = train_test_split(STDofDiff, target, test_size=0.25, random_state=0)\n  scaler = StandardScaler()\n  X_train=scaler.fit_transform(X_train)\n  X_test=scaler.fit_transform(X_test)\n  decision_tree = DecisionTreeClassifier(random_state=456)\n  decision_tree.fit(X_train, y_train)\n  y_pred = decision_tree.predict(X_test)\n  precision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\n  model.append(decision_tree)\n  t.append(ws)\n  fscoreVal.append(fscore)\ncustom_multiplot(\n    t, [fscoreVal],\n    title='Logarithmic Scale Plot',\n    xlabel='X-axis (log scale)',\n    ylabel='Y-axis (log scale)',\n    legend_labels=['y1', 'y2'],\n    colors=['blue', 'red'],\n    title_size=20,\n    label_font_size=14,\n    tick_font_size=12,\n    grid=True,\n    save_as='logarithmic_plot.png',\n    figsize=(10, 6),\n    xlog=False,\n    ylog=False\n)"
  },
  {
    "objectID": "posts/CV for Tie-Ballast/index.html",
    "href": "posts/CV for Tie-Ballast/index.html",
    "title": "CV for Tie-Ballast",
    "section": "",
    "text": "This study develops an effective system for in-motion and autonomous identification of crossties and ballast, toward implementing non-contact sensors that can evaluate the condition of railroad structures. An array of sensors that include distance LIDAR sensors, magnetic sensors, and cameras are employed to test a prototype system in motion, onboard a remotely controlled track cart that can travel at speeds of up to 10 mph. The tests are performed on revenue-service tracks and the measurements are used to develop autonomous post-processing approaches that can be readily adopted by the railroads. Two distinct techniques for the LIDAR sensors are explored. Next, a machine learning model is developed to achieve the task with potentially more accuracy. To this end, three machine learning models, using three types of inputs, are developed to identify the optimal model. The DecisionTree algorithm coupled with the standard deviation of the difference between two distance sensors proved to be the most effective.\nImport the required libraries"
  },
  {
    "objectID": "posts/CV for Tie-Ballast/index.html#labeling",
    "href": "posts/CV for Tie-Ballast/index.html#labeling",
    "title": "CV for Tie-Ballast",
    "section": "Labeling",
    "text": "Labeling\nBefore training the model, we need to manually label the images. For this purpose, a section of the dataset is chosen and labeled. The indices for all the ties in that specific section are saved in the following dataframe.\ntie_index = pd.read_excel('/content/drive/MyDrive/Vision/Huckelberry/March_6/1334/10fps/Training/tie_index.xlsx')\nlabel = np.zeros(df_new['image_index'].shape[0])\n\nfor i in tie_index['tie_index']:\n  label[i] = 1\n\ndf_new['labels'] = label\ndf_train = df_new[(df_new['Unnamed: 0']&gt;=47) & (df_new['Unnamed: 0']&lt;=193)]\nDefining the features\nfeatures = []\n\nfor i in range(0, len(df_train['processed_images'])):\n  features.append(np.array(cv2.resize(df_train.iloc[i]['processed_images'], (100, 500))))\n\n\nfeatures = np.array(features)\nDefining the labels\nlabels = df_train['labels']\nNormalizing the features\nfeatures = features.astype('float32') / 255.0"
  },
  {
    "objectID": "posts/CV for Tie-Ballast/index.html#traintest-split",
    "href": "posts/CV for Tie-Ballast/index.html#traintest-split",
    "title": "CV for Tie-Ballast",
    "section": "Train/Test Split",
    "text": "Train/Test Split\ntrain_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.2, stratify=labels)\nAccording to the block below we have 117 data points in training set and 30 data points in test set\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)\n(117, 500, 100, 3) (117,) (30, 500, 100, 3) (30,)\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Dense"
  },
  {
    "objectID": "posts/CV for Tie-Ballast/index.html#importing-inception-v3",
    "href": "posts/CV for Tie-Ballast/index.html#importing-inception-v3",
    "title": "CV for Tie-Ballast",
    "section": "Importing Inception V3",
    "text": "Importing Inception V3\nbase_model = InceptionV3(weights='imagenet', include_top=False,input_shape=(train_x.shape[1],train_x.shape[2],train_x.shape[3]))\nfor layer in base_model.layers:\n    layer.trainable = False\n\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\npredictions = Dense(1, activation='sigmoid')(x)\n\nImported_model = Model(inputs=base_model.input, outputs=predictions)\nImported_model.summary()\nModel: “model_1” __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_2 (InputLayer) [(None, 500, 100, 3)] 0 []\nconv2d_94 (Conv2D) (None, 249, 49, 32) 864 [‘input_2[0][0]’]\nbatch_normalization_94 (Ba (None, 249, 49, 32) 96 [‘conv2d_94[0][0]’] tchNormalization)\nactivation_94 (Activation) (None, 249, 49, 32) 0 [‘batch_normalization_94[0][0]’]\nconv2d_95 (Conv2D) (None, 247, 47, 32) 9216 [‘activation_94[0][0]’]\nbatch_normalization_95 (Ba (None, 247, 47, 32) 96 [‘conv2d_95[0][0]’] tchNormalization)\nactivation_95 (Activation) (None, 247, 47, 32) 0 [‘batch_normalization_95[0][0]’]\nconv2d_96 (Conv2D) (None, 247, 47, 64) 18432 [‘activation_95[0][0]’]\nbatch_normalization_96 (Ba (None, 247, 47, 64) 192 [‘conv2d_96[0][0]’] tchNormalization)\nactivation_96 (Activation) (None, 247, 47, 64) 0 [‘batch_normalization_96[0][0]’]\nmax_pooling2d_4 (MaxPoolin (None, 123, 23, 64) 0 [‘activation_96[0][0]’] g2D)\nconv2d_97 (Conv2D) (None, 123, 23, 80) 5120 [‘max_pooling2d_4[0][0]’]\nbatch_normalization_97 (Ba (None, 123, 23, 80) 240 [‘conv2d_97[0][0]’] tchNormalization)\nactivation_97 (Activation) (None, 123, 23, 80) 0 [‘batch_normalization_97[0][0]’]\nconv2d_98 (Conv2D) (None, 121, 21, 192) 138240 [‘activation_97[0][0]’]\nbatch_normalization_98 (Ba (None, 121, 21, 192) 576 [‘conv2d_98[0][0]’] tchNormalization)\nactivation_98 (Activation) (None, 121, 21, 192) 0 [‘batch_normalization_98[0][0]’]\nmax_pooling2d_5 (MaxPoolin (None, 60, 10, 192) 0 [‘activation_98[0][0]’] g2D)\nconv2d_102 (Conv2D) (None, 60, 10, 64) 12288 [‘max_pooling2d_5[0][0]’]\nbatch_normalization_102 (B (None, 60, 10, 64) 192 [‘conv2d_102[0][0]’] atchNormalization)\nactivation_102 (Activation (None, 60, 10, 64) 0 [‘batch_normalization_102[0][0 ) ]’]\nconv2d_100 (Conv2D) (None, 60, 10, 48) 9216 [‘max_pooling2d_5[0][0]’]\nconv2d_103 (Conv2D) (None, 60, 10, 96) 55296 [‘activation_102[0][0]’]\nbatch_normalization_100 (B (None, 60, 10, 48) 144 [‘conv2d_100[0][0]’] atchNormalization)\nbatch_normalization_103 (B (None, 60, 10, 96) 288 [‘conv2d_103[0][0]’] atchNormalization)\nactivation_100 (Activation (None, 60, 10, 48) 0 [‘batch_normalization_100[0][0 ) ]’]\nactivation_103 (Activation (None, 60, 10, 96) 0 [‘batch_normalization_103[0][0 ) ]’]\naverage_pooling2d_9 (Avera (None, 60, 10, 192) 0 [‘max_pooling2d_5[0][0]’] gePooling2D)\nconv2d_99 (Conv2D) (None, 60, 10, 64) 12288 [‘max_pooling2d_5[0][0]’]\nconv2d_101 (Conv2D) (None, 60, 10, 64) 76800 [‘activation_100[0][0]’]\nconv2d_104 (Conv2D) (None, 60, 10, 96) 82944 [‘activation_103[0][0]’]\nconv2d_105 (Conv2D) (None, 60, 10, 32) 6144 [‘average_pooling2d_9[0][0]’]\nbatch_normalization_99 (Ba (None, 60, 10, 64) 192 [‘conv2d_99[0][0]’] tchNormalization)\nbatch_normalization_101 (B (None, 60, 10, 64) 192 [‘conv2d_101[0][0]’] atchNormalization)\nbatch_normalization_104 (B (None, 60, 10, 96) 288 [‘conv2d_104[0][0]’] atchNormalization)\nbatch_normalization_105 (B (None, 60, 10, 32) 96 [‘conv2d_105[0][0]’] atchNormalization)\nactivation_99 (Activation) (None, 60, 10, 64) 0 [‘batch_normalization_99[0][0]’]\nactivation_101 (Activation (None, 60, 10, 64) 0 [‘batch_normalization_101[0][0 ) ]’]\nactivation_104 (Activation (None, 60, 10, 96) 0 [‘batch_normalization_104[0][0 ) ]’]\nactivation_105 (Activation (None, 60, 10, 32) 0 [‘batch_normalization_105[0][0 ) ]’]\nmixed0 (Concatenate) (None, 60, 10, 256) 0 [‘activation_99[0][0]’, ‘activation_101[0][0]’, ‘activation_104[0][0]’, ‘activation_105[0][0]’]\nconv2d_109 (Conv2D) (None, 60, 10, 64) 16384 [‘mixed0[0][0]’]\nbatch_normalization_109 (B (None, 60, 10, 64) 192 [‘conv2d_109[0][0]’] atchNormalization)\nactivation_109 (Activation (None, 60, 10, 64) 0 [‘batch_normalization_109[0][0 ) ]’]\nconv2d_107 (Conv2D) (None, 60, 10, 48) 12288 [‘mixed0[0][0]’]\nconv2d_110 (Conv2D) (None, 60, 10, 96) 55296 [‘activation_109[0][0]’]\nbatch_normalization_107 (B (None, 60, 10, 48) 144 [‘conv2d_107[0][0]’] atchNormalization)\nbatch_normalization_110 (B (None, 60, 10, 96) 288 [‘conv2d_110[0][0]’] atchNormalization)\nactivation_107 (Activation (None, 60, 10, 48) 0 [‘batch_normalization_107[0][0 ) ]’]\nactivation_110 (Activation (None, 60, 10, 96) 0 [‘batch_normalization_110[0][0 ) ]’]\naverage_pooling2d_10 (Aver (None, 60, 10, 256) 0 [‘mixed0[0][0]’] agePooling2D)\nconv2d_106 (Conv2D) (None, 60, 10, 64) 16384 [‘mixed0[0][0]’]\nconv2d_108 (Conv2D) (None, 60, 10, 64) 76800 [‘activation_107[0][0]’]\nconv2d_111 (Conv2D) (None, 60, 10, 96) 82944 [‘activation_110[0][0]’]\nconv2d_112 (Conv2D) (None, 60, 10, 64) 16384 [‘average_pooling2d_10[0][0]’]\nbatch_normalization_106 (B (None, 60, 10, 64) 192 [‘conv2d_106[0][0]’] atchNormalization)\nbatch_normalization_108 (B (None, 60, 10, 64) 192 [‘conv2d_108[0][0]’] atchNormalization)\nbatch_normalization_111 (B (None, 60, 10, 96) 288 [‘conv2d_111[0][0]’] atchNormalization)\nbatch_normalization_112 (B (None, 60, 10, 64) 192 [‘conv2d_112[0][0]’] atchNormalization)\nactivation_106 (Activation (None, 60, 10, 64) 0 [‘batch_normalization_106[0][0 ) ]’]\nactivation_108 (Activation (None, 60, 10, 64) 0 [‘batch_normalization_108[0][0 ) ]’]\nactivation_111 (Activation (None, 60, 10, 96) 0 [‘batch_normalization_111[0][0 ) ]’]\nactivation_112 (Activation (None, 60, 10, 64) 0 [‘batch_normalization_112[0][0 ) ]’]\nmixed1 (Concatenate) (None, 60, 10, 288) 0 [‘activation_106[0][0]’, ‘activation_108[0][0]’, ‘activation_111[0][0]’, ‘activation_112[0][0]’]\nconv2d_116 (Conv2D) (None, 60, 10, 64) 18432 [‘mixed1[0][0]’]\nbatch_normalization_116 (B (None, 60, 10, 64) 192 [‘conv2d_116[0][0]’] atchNormalization)\nactivation_116 (Activation (None, 60, 10, 64) 0 [‘batch_normalization_116[0][0 ) ]’]\nconv2d_114 (Conv2D) (None, 60, 10, 48) 13824 [‘mixed1[0][0]’]\nconv2d_117 (Conv2D) (None, 60, 10, 96) 55296 [‘activation_116[0][0]’]\nbatch_normalization_114 (B (None, 60, 10, 48) 144 [‘conv2d_114[0][0]’] atchNormalization)\nbatch_normalization_117 (B (None, 60, 10, 96) 288 [‘conv2d_117[0][0]’] atchNormalization)\nactivation_114 (Activation (None, 60, 10, 48) 0 [‘batch_normalization_114[0][0 ) ]’]\nactivation_117 (Activation (None, 60, 10, 96) 0 [‘batch_normalization_117[0][0 ) ]’]\naverage_pooling2d_11 (Aver (None, 60, 10, 288) 0 [‘mixed1[0][0]’] agePooling2D)\nconv2d_113 (Conv2D) (None, 60, 10, 64) 18432 [‘mixed1[0][0]’]\nconv2d_115 (Conv2D) (None, 60, 10, 64) 76800 [‘activation_114[0][0]’]\nconv2d_118 (Conv2D) (None, 60, 10, 96) 82944 [‘activation_117[0][0]’]\nconv2d_119 (Conv2D) (None, 60, 10, 64) 18432 [‘average_pooling2d_11[0][0]’]\nbatch_normalization_113 (B (None, 60, 10, 64) 192 [‘conv2d_113[0][0]’] atchNormalization)\nbatch_normalization_115 (B (None, 60, 10, 64) 192 [‘conv2d_115[0][0]’] atchNormalization)\nbatch_normalization_118 (B (None, 60, 10, 96) 288 [‘conv2d_118[0][0]’] atchNormalization)\nbatch_normalization_119 (B (None, 60, 10, 64) 192 [‘conv2d_119[0][0]’] atchNormalization)\nactivation_113 (Activation (None, 60, 10, 64) 0 [‘batch_normalization_113[0][0 ) ]’]\nactivation_115 (Activation (None, 60, 10, 64) 0 [‘batch_normalization_115[0][0 ) ]’]\nactivation_118 (Activation (None, 60, 10, 96) 0 [‘batch_normalization_118[0][0 ) ]’]\nactivation_119 (Activation (None, 60, 10, 64) 0 [‘batch_normalization_119[0][0 ) ]’]\nmixed2 (Concatenate) (None, 60, 10, 288) 0 [‘activation_113[0][0]’, ‘activation_115[0][0]’, ‘activation_118[0][0]’, ‘activation_119[0][0]’]\nconv2d_121 (Conv2D) (None, 60, 10, 64) 18432 [‘mixed2[0][0]’]\nbatch_normalization_121 (B (None, 60, 10, 64) 192 [‘conv2d_121[0][0]’] atchNormalization)\nactivation_121 (Activation (None, 60, 10, 64) 0 [‘batch_normalization_121[0][0 ) ]’]\nconv2d_122 (Conv2D) (None, 60, 10, 96) 55296 [‘activation_121[0][0]’]\nbatch_normalization_122 (B (None, 60, 10, 96) 288 [‘conv2d_122[0][0]’] atchNormalization)\nactivation_122 (Activation (None, 60, 10, 96) 0 [‘batch_normalization_122[0][0 ) ]’]\nconv2d_120 (Conv2D) (None, 29, 4, 384) 995328 [‘mixed2[0][0]’]\nconv2d_123 (Conv2D) (None, 29, 4, 96) 82944 [‘activation_122[0][0]’]\nbatch_normalization_120 (B (None, 29, 4, 384) 1152 [‘conv2d_120[0][0]’] atchNormalization)\nbatch_normalization_123 (B (None, 29, 4, 96) 288 [‘conv2d_123[0][0]’] atchNormalization)\nactivation_120 (Activation (None, 29, 4, 384) 0 [‘batch_normalization_120[0][0 ) ]’]\nactivation_123 (Activation (None, 29, 4, 96) 0 [‘batch_normalization_123[0][0 ) ]’]\nmax_pooling2d_6 (MaxPoolin (None, 29, 4, 288) 0 [‘mixed2[0][0]’] g2D)\nmixed3 (Concatenate) (None, 29, 4, 768) 0 [‘activation_120[0][0]’, ‘activation_123[0][0]’, ‘max_pooling2d_6[0][0]’]\nconv2d_128 (Conv2D) (None, 29, 4, 128) 98304 [‘mixed3[0][0]’]\nbatch_normalization_128 (B (None, 29, 4, 128) 384 [‘conv2d_128[0][0]’] atchNormalization)\nactivation_128 (Activation (None, 29, 4, 128) 0 [‘batch_normalization_128[0][0 ) ]’]\nconv2d_129 (Conv2D) (None, 29, 4, 128) 114688 [‘activation_128[0][0]’]\nbatch_normalization_129 (B (None, 29, 4, 128) 384 [‘conv2d_129[0][0]’] atchNormalization)\nactivation_129 (Activation (None, 29, 4, 128) 0 [‘batch_normalization_129[0][0 ) ]’]\nconv2d_125 (Conv2D) (None, 29, 4, 128) 98304 [‘mixed3[0][0]’]\nconv2d_130 (Conv2D) (None, 29, 4, 128) 114688 [‘activation_129[0][0]’]\nbatch_normalization_125 (B (None, 29, 4, 128) 384 [‘conv2d_125[0][0]’] atchNormalization)\nbatch_normalization_130 (B (None, 29, 4, 128) 384 [‘conv2d_130[0][0]’] atchNormalization)\nactivation_125 (Activation (None, 29, 4, 128) 0 [‘batch_normalization_125[0][0 ) ]’]\nactivation_130 (Activation (None, 29, 4, 128) 0 [‘batch_normalization_130[0][0 ) ]’]\nconv2d_126 (Conv2D) (None, 29, 4, 128) 114688 [‘activation_125[0][0]’]\nconv2d_131 (Conv2D) (None, 29, 4, 128) 114688 [‘activation_130[0][0]’]\nbatch_normalization_126 (B (None, 29, 4, 128) 384 [‘conv2d_126[0][0]’] atchNormalization)\nbatch_normalization_131 (B (None, 29, 4, 128) 384 [‘conv2d_131[0][0]’] atchNormalization)\nactivation_126 (Activation (None, 29, 4, 128) 0 [‘batch_normalization_126[0][0 ) ]’]\nactivation_131 (Activation (None, 29, 4, 128) 0 [‘batch_normalization_131[0][0 ) ]’]\naverage_pooling2d_12 (Aver (None, 29, 4, 768) 0 [‘mixed3[0][0]’] agePooling2D)\nconv2d_124 (Conv2D) (None, 29, 4, 192) 147456 [‘mixed3[0][0]’]\nconv2d_127 (Conv2D) (None, 29, 4, 192) 172032 [‘activation_126[0][0]’]\nconv2d_132 (Conv2D) (None, 29, 4, 192) 172032 [‘activation_131[0][0]’]\nconv2d_133 (Conv2D) (None, 29, 4, 192) 147456 [‘average_pooling2d_12[0][0]’]\nbatch_normalization_124 (B (None, 29, 4, 192) 576 [‘conv2d_124[0][0]’] atchNormalization)\nbatch_normalization_127 (B (None, 29, 4, 192) 576 [‘conv2d_127[0][0]’] atchNormalization)\nbatch_normalization_132 (B (None, 29, 4, 192) 576 [‘conv2d_132[0][0]’] atchNormalization)\nbatch_normalization_133 (B (None, 29, 4, 192) 576 [‘conv2d_133[0][0]’] atchNormalization)\nactivation_124 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_124[0][0 ) ]’]\nactivation_127 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_127[0][0 ) ]’]\nactivation_132 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_132[0][0 ) ]’]\nactivation_133 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_133[0][0 ) ]’]\nmixed4 (Concatenate) (None, 29, 4, 768) 0 [‘activation_124[0][0]’, ‘activation_127[0][0]’, ‘activation_132[0][0]’, ‘activation_133[0][0]’]\nconv2d_138 (Conv2D) (None, 29, 4, 160) 122880 [‘mixed4[0][0]’]\nbatch_normalization_138 (B (None, 29, 4, 160) 480 [‘conv2d_138[0][0]’] atchNormalization)\nactivation_138 (Activation (None, 29, 4, 160) 0 [‘batch_normalization_138[0][0 ) ]’]\nconv2d_139 (Conv2D) (None, 29, 4, 160) 179200 [‘activation_138[0][0]’]\nbatch_normalization_139 (B (None, 29, 4, 160) 480 [‘conv2d_139[0][0]’] atchNormalization)\nactivation_139 (Activation (None, 29, 4, 160) 0 [‘batch_normalization_139[0][0 ) ]’]\nconv2d_135 (Conv2D) (None, 29, 4, 160) 122880 [‘mixed4[0][0]’]\nconv2d_140 (Conv2D) (None, 29, 4, 160) 179200 [‘activation_139[0][0]’]\nbatch_normalization_135 (B (None, 29, 4, 160) 480 [‘conv2d_135[0][0]’] atchNormalization)\nbatch_normalization_140 (B (None, 29, 4, 160) 480 [‘conv2d_140[0][0]’] atchNormalization)\nactivation_135 (Activation (None, 29, 4, 160) 0 [‘batch_normalization_135[0][0 ) ]’]\nactivation_140 (Activation (None, 29, 4, 160) 0 [‘batch_normalization_140[0][0 ) ]’]\nconv2d_136 (Conv2D) (None, 29, 4, 160) 179200 [‘activation_135[0][0]’]\nconv2d_141 (Conv2D) (None, 29, 4, 160) 179200 [‘activation_140[0][0]’]\nbatch_normalization_136 (B (None, 29, 4, 160) 480 [‘conv2d_136[0][0]’] atchNormalization)\nbatch_normalization_141 (B (None, 29, 4, 160) 480 [‘conv2d_141[0][0]’] atchNormalization)\nactivation_136 (Activation (None, 29, 4, 160) 0 [‘batch_normalization_136[0][0 ) ]’]\nactivation_141 (Activation (None, 29, 4, 160) 0 [‘batch_normalization_141[0][0 ) ]’]\naverage_pooling2d_13 (Aver (None, 29, 4, 768) 0 [‘mixed4[0][0]’] agePooling2D)\nconv2d_134 (Conv2D) (None, 29, 4, 192) 147456 [‘mixed4[0][0]’]\nconv2d_137 (Conv2D) (None, 29, 4, 192) 215040 [‘activation_136[0][0]’]\nconv2d_142 (Conv2D) (None, 29, 4, 192) 215040 [‘activation_141[0][0]’]\nconv2d_143 (Conv2D) (None, 29, 4, 192) 147456 [‘average_pooling2d_13[0][0]’]\nbatch_normalization_134 (B (None, 29, 4, 192) 576 [‘conv2d_134[0][0]’] atchNormalization)\nbatch_normalization_137 (B (None, 29, 4, 192) 576 [‘conv2d_137[0][0]’] atchNormalization)\nbatch_normalization_142 (B (None, 29, 4, 192) 576 [‘conv2d_142[0][0]’] atchNormalization)\nbatch_normalization_143 (B (None, 29, 4, 192) 576 [‘conv2d_143[0][0]’] atchNormalization)\nactivation_134 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_134[0][0 ) ]’]\nactivation_137 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_137[0][0 ) ]’]\nactivation_142 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_142[0][0 ) ]’]\nactivation_143 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_143[0][0 ) ]’]\nmixed5 (Concatenate) (None, 29, 4, 768) 0 [‘activation_134[0][0]’, ‘activation_137[0][0]’, ‘activation_142[0][0]’, ‘activation_143[0][0]’]\nconv2d_148 (Conv2D) (None, 29, 4, 160) 122880 [‘mixed5[0][0]’]\nbatch_normalization_148 (B (None, 29, 4, 160) 480 [‘conv2d_148[0][0]’] atchNormalization)\nactivation_148 (Activation (None, 29, 4, 160) 0 [‘batch_normalization_148[0][0 ) ]’]\nconv2d_149 (Conv2D) (None, 29, 4, 160) 179200 [‘activation_148[0][0]’]\nbatch_normalization_149 (B (None, 29, 4, 160) 480 [‘conv2d_149[0][0]’] atchNormalization)\nactivation_149 (Activation (None, 29, 4, 160) 0 [‘batch_normalization_149[0][0 ) ]’]\nconv2d_145 (Conv2D) (None, 29, 4, 160) 122880 [‘mixed5[0][0]’]\nconv2d_150 (Conv2D) (None, 29, 4, 160) 179200 [‘activation_149[0][0]’]\nbatch_normalization_145 (B (None, 29, 4, 160) 480 [‘conv2d_145[0][0]’] atchNormalization)\nbatch_normalization_150 (B (None, 29, 4, 160) 480 [‘conv2d_150[0][0]’] atchNormalization)\nactivation_145 (Activation (None, 29, 4, 160) 0 [‘batch_normalization_145[0][0 ) ]’]\nactivation_150 (Activation (None, 29, 4, 160) 0 [‘batch_normalization_150[0][0 ) ]’]\nconv2d_146 (Conv2D) (None, 29, 4, 160) 179200 [‘activation_145[0][0]’]\nconv2d_151 (Conv2D) (None, 29, 4, 160) 179200 [‘activation_150[0][0]’]\nbatch_normalization_146 (B (None, 29, 4, 160) 480 [‘conv2d_146[0][0]’] atchNormalization)\nbatch_normalization_151 (B (None, 29, 4, 160) 480 [‘conv2d_151[0][0]’] atchNormalization)\nactivation_146 (Activation (None, 29, 4, 160) 0 [‘batch_normalization_146[0][0 ) ]’]\nactivation_151 (Activation (None, 29, 4, 160) 0 [‘batch_normalization_151[0][0 ) ]’]\naverage_pooling2d_14 (Aver (None, 29, 4, 768) 0 [‘mixed5[0][0]’] agePooling2D)\nconv2d_144 (Conv2D) (None, 29, 4, 192) 147456 [‘mixed5[0][0]’]\nconv2d_147 (Conv2D) (None, 29, 4, 192) 215040 [‘activation_146[0][0]’]\nconv2d_152 (Conv2D) (None, 29, 4, 192) 215040 [‘activation_151[0][0]’]\nconv2d_153 (Conv2D) (None, 29, 4, 192) 147456 [‘average_pooling2d_14[0][0]’]\nbatch_normalization_144 (B (None, 29, 4, 192) 576 [‘conv2d_144[0][0]’] atchNormalization)\nbatch_normalization_147 (B (None, 29, 4, 192) 576 [‘conv2d_147[0][0]’] atchNormalization)\nbatch_normalization_152 (B (None, 29, 4, 192) 576 [‘conv2d_152[0][0]’] atchNormalization)\nbatch_normalization_153 (B (None, 29, 4, 192) 576 [‘conv2d_153[0][0]’] atchNormalization)\nactivation_144 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_144[0][0 ) ]’]\nactivation_147 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_147[0][0 ) ]’]\nactivation_152 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_152[0][0 ) ]’]\nactivation_153 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_153[0][0 ) ]’]\nmixed6 (Concatenate) (None, 29, 4, 768) 0 [‘activation_144[0][0]’, ‘activation_147[0][0]’, ‘activation_152[0][0]’, ‘activation_153[0][0]’]\nconv2d_158 (Conv2D) (None, 29, 4, 192) 147456 [‘mixed6[0][0]’]\nbatch_normalization_158 (B (None, 29, 4, 192) 576 [‘conv2d_158[0][0]’] atchNormalization)\nactivation_158 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_158[0][0 ) ]’]\nconv2d_159 (Conv2D) (None, 29, 4, 192) 258048 [‘activation_158[0][0]’]\nbatch_normalization_159 (B (None, 29, 4, 192) 576 [‘conv2d_159[0][0]’] atchNormalization)\nactivation_159 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_159[0][0 ) ]’]\nconv2d_155 (Conv2D) (None, 29, 4, 192) 147456 [‘mixed6[0][0]’]\nconv2d_160 (Conv2D) (None, 29, 4, 192) 258048 [‘activation_159[0][0]’]\nbatch_normalization_155 (B (None, 29, 4, 192) 576 [‘conv2d_155[0][0]’] atchNormalization)\nbatch_normalization_160 (B (None, 29, 4, 192) 576 [‘conv2d_160[0][0]’] atchNormalization)\nactivation_155 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_155[0][0 ) ]’]\nactivation_160 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_160[0][0 ) ]’]\nconv2d_156 (Conv2D) (None, 29, 4, 192) 258048 [‘activation_155[0][0]’]\nconv2d_161 (Conv2D) (None, 29, 4, 192) 258048 [‘activation_160[0][0]’]\nbatch_normalization_156 (B (None, 29, 4, 192) 576 [‘conv2d_156[0][0]’] atchNormalization)\nbatch_normalization_161 (B (None, 29, 4, 192) 576 [‘conv2d_161[0][0]’] atchNormalization)\nactivation_156 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_156[0][0 ) ]’]\nactivation_161 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_161[0][0 ) ]’]\naverage_pooling2d_15 (Aver (None, 29, 4, 768) 0 [‘mixed6[0][0]’] agePooling2D)\nconv2d_154 (Conv2D) (None, 29, 4, 192) 147456 [‘mixed6[0][0]’]\nconv2d_157 (Conv2D) (None, 29, 4, 192) 258048 [‘activation_156[0][0]’]\nconv2d_162 (Conv2D) (None, 29, 4, 192) 258048 [‘activation_161[0][0]’]\nconv2d_163 (Conv2D) (None, 29, 4, 192) 147456 [‘average_pooling2d_15[0][0]’]\nbatch_normalization_154 (B (None, 29, 4, 192) 576 [‘conv2d_154[0][0]’] atchNormalization)\nbatch_normalization_157 (B (None, 29, 4, 192) 576 [‘conv2d_157[0][0]’] atchNormalization)\nbatch_normalization_162 (B (None, 29, 4, 192) 576 [‘conv2d_162[0][0]’] atchNormalization)\nbatch_normalization_163 (B (None, 29, 4, 192) 576 [‘conv2d_163[0][0]’] atchNormalization)\nactivation_154 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_154[0][0 ) ]’]\nactivation_157 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_157[0][0 ) ]’]\nactivation_162 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_162[0][0 ) ]’]\nactivation_163 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_163[0][0 ) ]’]\nmixed7 (Concatenate) (None, 29, 4, 768) 0 [‘activation_154[0][0]’, ‘activation_157[0][0]’, ‘activation_162[0][0]’, ‘activation_163[0][0]’]\nconv2d_166 (Conv2D) (None, 29, 4, 192) 147456 [‘mixed7[0][0]’]\nbatch_normalization_166 (B (None, 29, 4, 192) 576 [‘conv2d_166[0][0]’] atchNormalization)\nactivation_166 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_166[0][0 ) ]’]\nconv2d_167 (Conv2D) (None, 29, 4, 192) 258048 [‘activation_166[0][0]’]\nbatch_normalization_167 (B (None, 29, 4, 192) 576 [‘conv2d_167[0][0]’] atchNormalization)\nactivation_167 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_167[0][0 ) ]’]\nconv2d_164 (Conv2D) (None, 29, 4, 192) 147456 [‘mixed7[0][0]’]\nconv2d_168 (Conv2D) (None, 29, 4, 192) 258048 [‘activation_167[0][0]’]\nbatch_normalization_164 (B (None, 29, 4, 192) 576 [‘conv2d_164[0][0]’] atchNormalization)\nbatch_normalization_168 (B (None, 29, 4, 192) 576 [‘conv2d_168[0][0]’] atchNormalization)\nactivation_164 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_164[0][0 ) ]’]\nactivation_168 (Activation (None, 29, 4, 192) 0 [‘batch_normalization_168[0][0 ) ]’]\nconv2d_165 (Conv2D) (None, 14, 1, 320) 552960 [‘activation_164[0][0]’]\nconv2d_169 (Conv2D) (None, 14, 1, 192) 331776 [‘activation_168[0][0]’]\nbatch_normalization_165 (B (None, 14, 1, 320) 960 [‘conv2d_165[0][0]’] atchNormalization)\nbatch_normalization_169 (B (None, 14, 1, 192) 576 [‘conv2d_169[0][0]’] atchNormalization)\nactivation_165 (Activation (None, 14, 1, 320) 0 [‘batch_normalization_165[0][0 ) ]’]\nactivation_169 (Activation (None, 14, 1, 192) 0 [‘batch_normalization_169[0][0 ) ]’]\nmax_pooling2d_7 (MaxPoolin (None, 14, 1, 768) 0 [‘mixed7[0][0]’] g2D)\nmixed8 (Concatenate) (None, 14, 1, 1280) 0 [‘activation_165[0][0]’, ‘activation_169[0][0]’, ‘max_pooling2d_7[0][0]’]\nconv2d_174 (Conv2D) (None, 14, 1, 448) 573440 [‘mixed8[0][0]’]\nbatch_normalization_174 (B (None, 14, 1, 448) 1344 [‘conv2d_174[0][0]’] atchNormalization)\nactivation_174 (Activation (None, 14, 1, 448) 0 [‘batch_normalization_174[0][0 ) ]’]\nconv2d_171 (Conv2D) (None, 14, 1, 384) 491520 [‘mixed8[0][0]’]\nconv2d_175 (Conv2D) (None, 14, 1, 384) 1548288 [‘activation_174[0][0]’]\nbatch_normalization_171 (B (None, 14, 1, 384) 1152 [‘conv2d_171[0][0]’] atchNormalization)\nbatch_normalization_175 (B (None, 14, 1, 384) 1152 [‘conv2d_175[0][0]’] atchNormalization)\nactivation_171 (Activation (None, 14, 1, 384) 0 [‘batch_normalization_171[0][0 ) ]’]\nactivation_175 (Activation (None, 14, 1, 384) 0 [‘batch_normalization_175[0][0 ) ]’]\nconv2d_172 (Conv2D) (None, 14, 1, 384) 442368 [‘activation_171[0][0]’]\nconv2d_173 (Conv2D) (None, 14, 1, 384) 442368 [‘activation_171[0][0]’]\nconv2d_176 (Conv2D) (None, 14, 1, 384) 442368 [‘activation_175[0][0]’]\nconv2d_177 (Conv2D) (None, 14, 1, 384) 442368 [‘activation_175[0][0]’]\naverage_pooling2d_16 (Aver (None, 14, 1, 1280) 0 [‘mixed8[0][0]’] agePooling2D)\nconv2d_170 (Conv2D) (None, 14, 1, 320) 409600 [‘mixed8[0][0]’]\nbatch_normalization_172 (B (None, 14, 1, 384) 1152 [‘conv2d_172[0][0]’] atchNormalization)\nbatch_normalization_173 (B (None, 14, 1, 384) 1152 [‘conv2d_173[0][0]’] atchNormalization)\nbatch_normalization_176 (B (None, 14, 1, 384) 1152 [‘conv2d_176[0][0]’] atchNormalization)\nbatch_normalization_177 (B (None, 14, 1, 384) 1152 [‘conv2d_177[0][0]’] atchNormalization)\nconv2d_178 (Conv2D) (None, 14, 1, 192) 245760 [‘average_pooling2d_16[0][0]’]\nbatch_normalization_170 (B (None, 14, 1, 320) 960 [‘conv2d_170[0][0]’] atchNormalization)\nactivation_172 (Activation (None, 14, 1, 384) 0 [‘batch_normalization_172[0][0 ) ]’]\nactivation_173 (Activation (None, 14, 1, 384) 0 [‘batch_normalization_173[0][0 ) ]’]\nactivation_176 (Activation (None, 14, 1, 384) 0 [‘batch_normalization_176[0][0 ) ]’]\nactivation_177 (Activation (None, 14, 1, 384) 0 [‘batch_normalization_177[0][0 ) ]’]\nbatch_normalization_178 (B (None, 14, 1, 192) 576 [‘conv2d_178[0][0]’] atchNormalization)\nactivation_170 (Activation (None, 14, 1, 320) 0 [‘batch_normalization_170[0][0 ) ]’]\nmixed9_0 (Concatenate) (None, 14, 1, 768) 0 [‘activation_172[0][0]’, ‘activation_173[0][0]’]\nconcatenate_2 (Concatenate (None, 14, 1, 768) 0 [‘activation_176[0][0]’, ) ‘activation_177[0][0]’]\nactivation_178 (Activation (None, 14, 1, 192) 0 [‘batch_normalization_178[0][0 ) ]’]\nmixed9 (Concatenate) (None, 14, 1, 2048) 0 [‘activation_170[0][0]’, ‘mixed9_0[0][0]’, ‘concatenate_2[0][0]’, ‘activation_178[0][0]’]\nconv2d_183 (Conv2D) (None, 14, 1, 448) 917504 [‘mixed9[0][0]’]\nbatch_normalization_183 (B (None, 14, 1, 448) 1344 [‘conv2d_183[0][0]’] atchNormalization)\nactivation_183 (Activation (None, 14, 1, 448) 0 [‘batch_normalization_183[0][0 ) ]’]\nconv2d_180 (Conv2D) (None, 14, 1, 384) 786432 [‘mixed9[0][0]’]\nconv2d_184 (Conv2D) (None, 14, 1, 384) 1548288 [‘activation_183[0][0]’]\nbatch_normalization_180 (B (None, 14, 1, 384) 1152 [‘conv2d_180[0][0]’] atchNormalization)\nbatch_normalization_184 (B (None, 14, 1, 384) 1152 [‘conv2d_184[0][0]’] atchNormalization)\nactivation_180 (Activation (None, 14, 1, 384) 0 [‘batch_normalization_180[0][0 ) ]’]\nactivation_184 (Activation (None, 14, 1, 384) 0 [‘batch_normalization_184[0][0 ) ]’]\nconv2d_181 (Conv2D) (None, 14, 1, 384) 442368 [‘activation_180[0][0]’]\nconv2d_182 (Conv2D) (None, 14, 1, 384) 442368 [‘activation_180[0][0]’]\nconv2d_185 (Conv2D) (None, 14, 1, 384) 442368 [‘activation_184[0][0]’]\nconv2d_186 (Conv2D) (None, 14, 1, 384) 442368 [‘activation_184[0][0]’]\naverage_pooling2d_17 (Aver (None, 14, 1, 2048) 0 [‘mixed9[0][0]’] agePooling2D)\nconv2d_179 (Conv2D) (None, 14, 1, 320) 655360 [‘mixed9[0][0]’]\nbatch_normalization_181 (B (None, 14, 1, 384) 1152 [‘conv2d_181[0][0]’] atchNormalization)\nbatch_normalization_182 (B (None, 14, 1, 384) 1152 [‘conv2d_182[0][0]’] atchNormalization)\nbatch_normalization_185 (B (None, 14, 1, 384) 1152 [‘conv2d_185[0][0]’] atchNormalization)\nbatch_normalization_186 (B (None, 14, 1, 384) 1152 [‘conv2d_186[0][0]’] atchNormalization)\nconv2d_187 (Conv2D) (None, 14, 1, 192) 393216 [‘average_pooling2d_17[0][0]’]\nbatch_normalization_179 (B (None, 14, 1, 320) 960 [‘conv2d_179[0][0]’] atchNormalization)\nactivation_181 (Activation (None, 14, 1, 384) 0 [‘batch_normalization_181[0][0 ) ]’]\nactivation_182 (Activation (None, 14, 1, 384) 0 [‘batch_normalization_182[0][0 ) ]’]\nactivation_185 (Activation (None, 14, 1, 384) 0 [‘batch_normalization_185[0][0 ) ]’]\nactivation_186 (Activation (None, 14, 1, 384) 0 [‘batch_normalization_186[0][0 ) ]’]\nbatch_normalization_187 (B (None, 14, 1, 192) 576 [‘conv2d_187[0][0]’] atchNormalization)\nactivation_179 (Activation (None, 14, 1, 320) 0 [‘batch_normalization_179[0][0 ) ]’]\nmixed9_1 (Concatenate) (None, 14, 1, 768) 0 [‘activation_181[0][0]’, ‘activation_182[0][0]’]\nconcatenate_3 (Concatenate (None, 14, 1, 768) 0 [‘activation_185[0][0]’, ) ‘activation_186[0][0]’]\nactivation_187 (Activation (None, 14, 1, 192) 0 [‘batch_normalization_187[0][0 ) ]’]\nmixed10 (Concatenate) (None, 14, 1, 2048) 0 [‘activation_179[0][0]’, ‘mixed9_1[0][0]’, ‘concatenate_3[0][0]’, ‘activation_187[0][0]’]\nglobal_average_pooling2d_1 (None, 2048) 0 [‘mixed10[0][0]’] (GlobalAveragePooling2D)\ndense_2 (Dense) (None, 1024) 2098176 [‘global_average_pooling2d_1[0 ][0]’]\ndense_3 (Dense) (None, 1) 1025 [‘dense_2[0][0]’]\n================================================================================================== Total params: 23901985 (91.18 MB) Trainable params: 2099201 (8.01 MB) Non-trainable params: 21802784 (83.17 MB) _____________________________________________________________________________________________\nAdam is our optimizer and binary_crossentropy is the loss function\nImported_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nImported_model.fit(train_x, train_y, epochs = 5, batch_size = 256, verbose=1, validation_data=(test_x, test_y))"
  },
  {
    "objectID": "posts/CV for Tie-Ballast/index.html#save-load-the-model",
    "href": "posts/CV for Tie-Ballast/index.html#save-load-the-model",
    "title": "CV for Tie-Ballast",
    "section": "Save / Load the model",
    "text": "Save / Load the model\nImported_model.save('/content/drive/MyDrive/Vision/Huckelberry/March_6/1334/10fps/Training/myModel.h5')\n#Imported_model.save_weights(\"myModel2.h5\", '/content/drive/MyDrive/Vision/Huckelberry/March_6/1334/10fps/Training')\nfrom keras.models import load_model\nImported_model = load_model('/content/drive/MyDrive/Vision/Huckelberry/March_6/1334/10fps/Training/myModel.h5')\n#new_model.load_weights(\"myModel2.h5\")"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "In this study, two non-contact velocity meters (Doppler LiDAR sensors) are mounted onboard a hi-rail vehicle and measured and stored the lateral vibration of both rails (right and left). The test is about 3.2 miles.\nSeveral steps had been taken which will non be discusseed here. In nutshel,\n\nData cleaning\nScaling the velocities\nHigh-pass filtering\nSolve for homoscedasticity\nAnd finally segmenting since we want to look at section of track not a single data point\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn import preprocessing\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support as score\ndef interactive_plot_scatter(df, x_axis, y_axis):\n  import plotly.express as px\n\n  fig = px.scatter(df, x_axis, y_axis)\n  return fig.show()\n\ndef interactive_plot_line(df, x_axis, y_axis):\n  import plotly.express as px\n\n  fig = px.line(df, x_axis, y_axis)\n  return fig.show()\n\ndef tach_cleaning(df):\n  tach = np.zeros(df['Tach'].shape[0])\n  for i in range(1, df['Tach'].shape[0]):\n    if df.iloc[i]['Tach'] &gt; 2.2:\n      tach[i] = 5\n  df['Tach_p'] = tach\n  return df\n\ndef position_string(df: dict) -&gt; str:\n  df['Pos'] = 0.000000\n  pos = df['Pos']\n  c = 0;\n  delta = float((1/36) * 7.25 * np.pi / 12);\n  for i in range(1, pos.shape[0]):\n    if (np.abs(df.iloc[i]['Tach']-df.iloc[i-1]['Tach'])&gt;1):\n      c = c + 1\n    pos[i] = delta * c\n  return pos\n\ndef remove_Keyence_dropout(df):\n  df = df[df['left_disp']&lt;1.15]\n  df = df[df['right_disp']&lt;0.925]\n  return df\n\ndef remove_outliers(df):\n  Q1 = np.percentile(df['right_disp'], 25,\n                   interpolation = 'midpoint')\n\n  Q3 = np.percentile(df['right_disp'], 75,\n                   interpolation = 'midpoint')\n  IQR = Q3 - Q1\n\n  up = Q3+1.5*IQR\n  low = Q1-1.5*IQR\n\n  df = df[df['right_disp']&lt;up]\n  df = df[df['right_disp']&gt;low]\n\n  Q1 = np.percentile(df['left_disp'], 25,\n                   interpolation = 'midpoint')\n\n  Q3 = np.percentile(df['left_disp'], 75,\n                   interpolation = 'midpoint')\n  IQR = Q3 - Q1\n\n  up = Q3+1.5*IQR\n  low = Q1-1.5*IQR\n\n  df = df[df['left_disp']&lt;up]\n  df = df[df['left_disp']&gt;low]\n  return df\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef custom_multiplot(x, y_sets, title=None, xlabel=None, ylabel=None, legend_labels=None, legend_loc='best', grid=True, save_as=None, colors=None, title_size=16, label_font_size=12, tick_font_size=10, figsize=(10, 6), xlim=None, ylim=None, xlog=False, ylog=False, scatter=False):\n    \"\"\"\n    Create a customized plot with multiple y-axis parameters using Matplotlib.\n\n    Parameters:\n    - x: x-axis data (list or NumPy array)\n    - y_sets: List of y-axis data sets (list of lists or NumPy arrays)\n    - title: Plot title (string, optional)\n    - xlabel: Label for the x-axis (string, optional)\n    - ylabel: Label for the y-axis (string, optional)\n    - legend_labels: Labels for the legend (list of strings, optional)\n    - legend_loc: Location of the legend ('best', 'upper left', 'upper right', 'lower left', 'lower right', etc.)\n    - grid: Display grid lines (boolean, optional)\n    - save_as: File name to save the plot as an image (string, optional)\n    - colors: List of line colors (list of strings or tuples, optional)\n    - title_size: Font size for the plot title (int, optional)\n    - label_font_size: Font size for axis labels and legend (int, optional)\n    - tick_font_size: Font size for tick labels (int, optional)\n    - figsize: Figure size as a tuple (width, height) (optional)\n    - xlim: Tuple specifying the x-axis limits (e.g., (xmin, xmax)) (optional)\n    - ylim: Tuple specifying the y-axis limits (e.g., (ymin, ymax)) (optional)\n    - xlog: Enable logarithmic scaling for the x-axis (boolean, optional)\n    - ylog: Enable logarithmic scaling for the y-axis (boolean, optional)\n\n    Returns:\n    - None\n    \"\"\"\n    plt.figure(figsize=figsize)  # Adjust the figure size\n\n    if colors is None:\n        colors = ['blue', 'red', 'green', 'purple', 'orange', 'cyan', 'magenta']\n\n    for i, y in enumerate(y_sets):\n        color = colors[i % len(colors)]\n        label = legend_labels[i] if legend_labels and i &lt; len(legend_labels) else None\n\n        if scatter:\n            plt.scatter(x, y, label=label, color=color, s=30)\n        elif xlog:\n            plt.semilogx(x, y, label=label, color=color, linewidth=2)\n        elif ylog:\n            plt.semilogy(x, y, label=label, color=color, linewidth=2)\n        else:\n            plt.plot(x, y, label=label, color=color, linewidth=2)\n\n    if legend_labels:\n        plt.legend(legend_labels, loc=legend_loc, fontsize=label_font_size)\n\n    if title:\n        plt.title(title, fontsize=title_size)\n\n    if xlabel:\n        plt.xlabel(xlabel, fontsize=label_font_size)\n\n    if ylabel:\n        plt.ylabel(ylabel, fontsize=label_font_size)\n\n    if grid:\n        plt.grid(True)\n\n    if xlim:\n        plt.xlim(xlim)\n\n    if ylim:\n        plt.ylim(ylim)\n\n    plt.xticks(fontsize=tick_font_size)\n    plt.yticks(fontsize=tick_font_size)\n\n    if xlog:\n        plt.xscale('log')\n    if ylog:\n        plt.yscale('log')\n    ax = plt.gca()\n    #ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    #ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n    if save_as:\n        plt.savefig(save_as, dpi=300, bbox_inches='tight')\n\n    plt.show()\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------\nIn the dataset we have two parameters. Fist one +2 sigma right/left velocity within the entire data and the other one is +2 sigma right/left velocity within each segment.\ndf = pd.read_csv('/content/drive/MyDrive/ML/SLineNS2.csv')\nIn here, we will only keep the maximum value of right or left rail for each parameter. The reason is that in this case if any of the left or right rail is loose it will mark it as loose.\ndef max_FromAll(row):\n    return max(row['Right_Plus_2Sigma_FromAll_Lateral_Weighted'], row['Left_Plus_2Sigma_FromAll_Lateral_Weighted'])\ndef max_FromSegment(row):\n    return max(row['Right_Plus_2Sigma_FromSegment_Lateral_Weighted'], row['Left_Plus_2Sigma_FromSegment_Lateral_Weighted'])\n\n# Apply the function to create a new column 'max_value'\ndf['max_value_FromAll'] = df.apply(max_FromAll, axis=1)\ndf['max_value_FromSegment'] = df.apply(max_FromSegment, axis=1)\n\nX = df[['max_value_FromAll', 'max_value_FromSegment']]\nn_clusters = 4 # we need 5 clusters based on elbow method\n\ngmm = GaussianMixture(n_components=5, random_state=42)\ngmm.fit(X)\ndf['Cluster'] = gmm.predict(X)\nsns.scatterplot(x='max_value_FromAll', y='max_value_FromSegment', hue='Cluster', data=df, palette=['green', 'red', 'purple', 'brown', 'blue'])\nplt.title('Gaussian Mixture Model Clustering')\nplt.show()\n\nsns.scatterplot(x='Segment', y='max_value_FromAll', hue='Cluster', data=df, palette=['green', 'red', 'purple', 'brown', 'blue'])\nplt.title('Gaussian Mixture Model Clustering')\nplt.show()\n\n\n\nAfter inspecting all the clusters in Google map, Red cluster (only segment) turned out to be a railroad switch"
  },
  {
    "objectID": "posts/Clustering/index.html#interpreting-the-results",
    "href": "posts/Clustering/index.html#interpreting-the-results",
    "title": "Clustering",
    "section": "",
    "text": "After inspecting all the clusters in Google map, Red cluster (only segment) turned out to be a railroad switch"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MortezaML",
    "section": "",
    "text": "S. Morteza H. Mirzaei\nsmhajimirzaei@vt.edu\nS. Morteza (Morty) H. Mirzaei is a dedicated and accomplished graduate research assistant at the Center for Vehicle Systems and Safety at Virginia Tech. He earned his Bachelor of Science degree in Mechanical Engineering from the esteemed University of Tehran in 2022, where he developed a strong foundation in engineering principles.\nMorty’s expertise lies in the exciting realm of vehicle dynamics, fault detection, data analysis, machine learning, sensor applications, and energy harvesting. He thrives on solving complex problems and exploring cutting-edge technologies that can make transportation safer and more efficient.\nCurrently, Morty is actively engaged in two exciting projects. The first project focuses on non-contact in-motion track instability detection using Doppler LiDAR sensors and machine learning algorithms. By exploring innovative sensing technologies, he aims to contribute to the development of safer and more efficient rail transits.\nIn his second project, Morty is involved in non-contact detection and evaluation of rail gage face grease through optical sensing methods. His work in this area promises to improve the maintenance and reliability of rail systems, ensuring smoother operations and reducing potential risks.\nAs he continues to advance his research career, Morty remains committed to leveraging technology and data-driven approaches to address real-world challenges in transportation.\nLinkedIn profile: https://www.linkedin.com/in/s-morteza-h-mirzaei/\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nMorteza Mirzaei\n\n\n\n\n\n\n  \n\n\n\n\nCV for Lubricity\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nMorteza Mirzaei\n\n\n\n\n\n\n  \n\n\n\n\nCV for Tie-Ballast\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nMorteza Mirzaei\n\n\n\n\n\n\n  \n\n\n\n\nReinforcement Learning\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nMorteza Mirzaei\n\n\n\n\n\n\n  \n\n\n\n\nTie-Ballast Identification\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nMorteza Mirzaei\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/CV for Lubricity/index.html",
    "href": "posts/CV for Lubricity/index.html",
    "title": "CV for Lubricity",
    "section": "",
    "text": "In this project, a camera was mounted on a rail cart targeted the gage face of the rail. The recorded videos then were processed to autonomously identify lubricated sections\nfrom google.colab import drive\ndrive.mount('/content/drive')\ndef interactive_plot_scatter(df, x_axis, y_axis, xlabel, ylabel):\n  import plotly.express as px\n\n  fig = px.scatter(df, x_axis, y_axis).update_layout(xaxis_title=xlabel, yaxis_title=ylabel)\n\n  return fig.show()\n\ndef interactive_plot_line(df, x_axis, y_axis, xlabel, ylabel):\n  import plotly.express as px\n\n  fig = px.line(df, x_axis, y_axis).update_layout(xaxis_title=xlabel, yaxis_title=ylabel)\n\n  return fig.show()\n!pip install moviepy\n!pip3 install imageio==2.4.1\n!pip install --upgrade imageio-ffmpeg\nimport moviepy.editor as mp\nimport cv2\nimport os\nimport numpy as np\nimport pandas as pd\nfrom google.colab.patches import cv2_imshow\nfrom numpy import genfromtxt\nfrom PIL import Image\nimport os\nfrom moviepy.editor import *\nTest = 'Test_1'\nCamera = 'A'\nEncoder = 'MP4'\nvideo_path = '/content/drive/MyDrive/RTL_Lubricity/Machine_Vision/Hucklebery/6_8_2023/' + Test + '/' + Test + '_' + Camera + '.' + Encoder\nsave_path = '/content/drive/MyDrive/RTL_Lubricity/Machine_Vision/Hucklebery/6_8_2023/' + Test +'/Reduced_size/' + Test + '_' + Camera + '.' + Encoder"
  },
  {
    "objectID": "posts/CV for Lubricity/index.html#lubricity-detection-using-computer-vision",
    "href": "posts/CV for Lubricity/index.html#lubricity-detection-using-computer-vision",
    "title": "CV for Lubricity",
    "section": "",
    "text": "In this project, a camera was mounted on a rail cart targeted the gage face of the rail. The recorded videos then were processed to autonomously identify lubricated sections\nfrom google.colab import drive\ndrive.mount('/content/drive')\ndef interactive_plot_scatter(df, x_axis, y_axis, xlabel, ylabel):\n  import plotly.express as px\n\n  fig = px.scatter(df, x_axis, y_axis).update_layout(xaxis_title=xlabel, yaxis_title=ylabel)\n\n  return fig.show()\n\ndef interactive_plot_line(df, x_axis, y_axis, xlabel, ylabel):\n  import plotly.express as px\n\n  fig = px.line(df, x_axis, y_axis).update_layout(xaxis_title=xlabel, yaxis_title=ylabel)\n\n  return fig.show()\n!pip install moviepy\n!pip3 install imageio==2.4.1\n!pip install --upgrade imageio-ffmpeg\nimport moviepy.editor as mp\nimport cv2\nimport os\nimport numpy as np\nimport pandas as pd\nfrom google.colab.patches import cv2_imshow\nfrom numpy import genfromtxt\nfrom PIL import Image\nimport os\nfrom moviepy.editor import *\nTest = 'Test_1'\nCamera = 'A'\nEncoder = 'MP4'\nvideo_path = '/content/drive/MyDrive/RTL_Lubricity/Machine_Vision/Hucklebery/6_8_2023/' + Test + '/' + Test + '_' + Camera + '.' + Encoder\nsave_path = '/content/drive/MyDrive/RTL_Lubricity/Machine_Vision/Hucklebery/6_8_2023/' + Test +'/Reduced_size/' + Test + '_' + Camera + '.' + Encoder"
  },
  {
    "objectID": "posts/CV for Lubricity/index.html#auto-labeling",
    "href": "posts/CV for Lubricity/index.html#auto-labeling",
    "title": "CV for Lubricity",
    "section": "Auto Labeling",
    "text": "Auto Labeling\nTwo sections of the video were lubricated. We manually labeled part of them for training.\nL = [886, 1379, 1792, 2809]     # L = ['Start of Lubrication' : index number, 'End of Lubrication' : index number, /////]\nendpoint = len(df_new)\nstep = 10\n\nlabels = np.ones(endpoint) * (-1)\n\nfor i in range(0, endpoint, step):\n  labels[i] = 0\n  for j in range(int(len(L) / 2)):\n    if (df_new.iloc[i]['Unnamed: 0']&gt;L[j*2]) and (df_new.iloc[i]['Unnamed: 0']&lt;L[j*2+1]):\n      labels[i] = 1\n\ndf_new['labels'] = labels\ndf_train = df_new[df_new['labels'] != -1]\nfeatures = []\n\nfor i in range(0, len(df_train['processed_images'])):\n  features.append(np.array(df_train.iloc[i]['processed_images']))\n\nfeatures = np.array(features)\n\nlabels = df_train['labels']\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\nfeatures = features.astype('float32') / 255.0\ntrain_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.2, stratify=labels)\n\ntrain_x, train_y = shuffle(train_x, train_y, random_state=0)\ntest_x, test_y = shuffle(test_x, test_y, random_state=0)\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)\n(269, 100, 100, 3) (269,) (68, 100, 100, 3) (68,)\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Dense\nbase_model = InceptionV3(weights='imagenet', include_top=False,input_shape=df_train.iloc[0]['processed_images'].shape)\nfor layer in base_model.layers:\n    layer.trainable = False\n\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\npredictions = Dense(1, activation='sigmoid')(x)\n\nImported_model = Model(inputs=base_model.input, outputs=predictions)\nImported_model.summary()\nModel: “model_1” __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_2 (InputLayer) [(None, 100, 100, 3 0 [] )]\nconv2d_94 (Conv2D) (None, 49, 49, 32) 864 [‘input_2[0][0]’]\nbatch_normalization_94 (BatchN (None, 49, 49, 32) 96 [‘conv2d_94[0][0]’] ormalization)\nactivation_94 (Activation) (None, 49, 49, 32) 0 [‘batch_normalization_94[0][0]’]\nconv2d_95 (Conv2D) (None, 47, 47, 32) 9216 [‘activation_94[0][0]’]\nbatch_normalization_95 (BatchN (None, 47, 47, 32) 96 [‘conv2d_95[0][0]’] ormalization)\nactivation_95 (Activation) (None, 47, 47, 32) 0 [‘batch_normalization_95[0][0]’]\nconv2d_96 (Conv2D) (None, 47, 47, 64) 18432 [‘activation_95[0][0]’]\nbatch_normalization_96 (BatchN (None, 47, 47, 64) 192 [‘conv2d_96[0][0]’] ormalization)\nactivation_96 (Activation) (None, 47, 47, 64) 0 [‘batch_normalization_96[0][0]’]\nmax_pooling2d_4 (MaxPooling2D) (None, 23, 23, 64) 0 [‘activation_96[0][0]’]\nconv2d_97 (Conv2D) (None, 23, 23, 80) 5120 [‘max_pooling2d_4[0][0]’]\nbatch_normalization_97 (BatchN (None, 23, 23, 80) 240 [‘conv2d_97[0][0]’] ormalization)\nactivation_97 (Activation) (None, 23, 23, 80) 0 [‘batch_normalization_97[0][0]’]\nconv2d_98 (Conv2D) (None, 21, 21, 192) 138240 [‘activation_97[0][0]’]\nbatch_normalization_98 (BatchN (None, 21, 21, 192) 576 [‘conv2d_98[0][0]’] ormalization)\nactivation_98 (Activation) (None, 21, 21, 192) 0 [‘batch_normalization_98[0][0]’]\nmax_pooling2d_5 (MaxPooling2D) (None, 10, 10, 192) 0 [‘activation_98[0][0]’]\nconv2d_102 (Conv2D) (None, 10, 10, 64) 12288 [‘max_pooling2d_5[0][0]’]\nbatch_normalization_102 (Batch (None, 10, 10, 64) 192 [‘conv2d_102[0][0]’] Normalization)\nactivation_102 (Activation) (None, 10, 10, 64) 0 [‘batch_normalization_102[0][0]’]\nconv2d_100 (Conv2D) (None, 10, 10, 48) 9216 [‘max_pooling2d_5[0][0]’]\nconv2d_103 (Conv2D) (None, 10, 10, 96) 55296 [‘activation_102[0][0]’]\nbatch_normalization_100 (Batch (None, 10, 10, 48) 144 [‘conv2d_100[0][0]’] Normalization)\nbatch_normalization_103 (Batch (None, 10, 10, 96) 288 [‘conv2d_103[0][0]’] Normalization)\nactivation_100 (Activation) (None, 10, 10, 48) 0 [‘batch_normalization_100[0][0]’]\nactivation_103 (Activation) (None, 10, 10, 96) 0 [‘batch_normalization_103[0][0]’]\naverage_pooling2d_9 (AveragePo (None, 10, 10, 192) 0 [‘max_pooling2d_5[0][0]’] oling2D)\nconv2d_99 (Conv2D) (None, 10, 10, 64) 12288 [‘max_pooling2d_5[0][0]’]\nconv2d_101 (Conv2D) (None, 10, 10, 64) 76800 [‘activation_100[0][0]’]\nconv2d_104 (Conv2D) (None, 10, 10, 96) 82944 [‘activation_103[0][0]’]\nconv2d_105 (Conv2D) (None, 10, 10, 32) 6144 [‘average_pooling2d_9[0][0]’]\nbatch_normalization_99 (BatchN (None, 10, 10, 64) 192 [‘conv2d_99[0][0]’] ormalization)\nbatch_normalization_101 (Batch (None, 10, 10, 64) 192 [‘conv2d_101[0][0]’] Normalization)\nbatch_normalization_104 (Batch (None, 10, 10, 96) 288 [‘conv2d_104[0][0]’] Normalization)\nbatch_normalization_105 (Batch (None, 10, 10, 32) 96 [‘conv2d_105[0][0]’] Normalization)\nactivation_99 (Activation) (None, 10, 10, 64) 0 [‘batch_normalization_99[0][0]’]\nactivation_101 (Activation) (None, 10, 10, 64) 0 [‘batch_normalization_101[0][0]’]\nactivation_104 (Activation) (None, 10, 10, 96) 0 [‘batch_normalization_104[0][0]’]\nactivation_105 (Activation) (None, 10, 10, 32) 0 [‘batch_normalization_105[0][0]’]\nmixed0 (Concatenate) (None, 10, 10, 256) 0 [‘activation_99[0][0]’, ‘activation_101[0][0]’, ‘activation_104[0][0]’, ‘activation_105[0][0]’]\nconv2d_109 (Conv2D) (None, 10, 10, 64) 16384 [‘mixed0[0][0]’]\nbatch_normalization_109 (Batch (None, 10, 10, 64) 192 [‘conv2d_109[0][0]’] Normalization)\nactivation_109 (Activation) (None, 10, 10, 64) 0 [‘batch_normalization_109[0][0]’]\nconv2d_107 (Conv2D) (None, 10, 10, 48) 12288 [‘mixed0[0][0]’]\nconv2d_110 (Conv2D) (None, 10, 10, 96) 55296 [‘activation_109[0][0]’]\nbatch_normalization_107 (Batch (None, 10, 10, 48) 144 [‘conv2d_107[0][0]’] Normalization)\nbatch_normalization_110 (Batch (None, 10, 10, 96) 288 [‘conv2d_110[0][0]’] Normalization)\nactivation_107 (Activation) (None, 10, 10, 48) 0 [‘batch_normalization_107[0][0]’]\nactivation_110 (Activation) (None, 10, 10, 96) 0 [‘batch_normalization_110[0][0]’]\naverage_pooling2d_10 (AverageP (None, 10, 10, 256) 0 [‘mixed0[0][0]’] ooling2D)\nconv2d_106 (Conv2D) (None, 10, 10, 64) 16384 [‘mixed0[0][0]’]\nconv2d_108 (Conv2D) (None, 10, 10, 64) 76800 [‘activation_107[0][0]’]\nconv2d_111 (Conv2D) (None, 10, 10, 96) 82944 [‘activation_110[0][0]’]\nconv2d_112 (Conv2D) (None, 10, 10, 64) 16384 [‘average_pooling2d_10[0][0]’]\nbatch_normalization_106 (Batch (None, 10, 10, 64) 192 [‘conv2d_106[0][0]’] Normalization)\nbatch_normalization_108 (Batch (None, 10, 10, 64) 192 [‘conv2d_108[0][0]’] Normalization)\nbatch_normalization_111 (Batch (None, 10, 10, 96) 288 [‘conv2d_111[0][0]’] Normalization)\nbatch_normalization_112 (Batch (None, 10, 10, 64) 192 [‘conv2d_112[0][0]’] Normalization)\nactivation_106 (Activation) (None, 10, 10, 64) 0 [‘batch_normalization_106[0][0]’]\nactivation_108 (Activation) (None, 10, 10, 64) 0 [‘batch_normalization_108[0][0]’]\nactivation_111 (Activation) (None, 10, 10, 96) 0 [‘batch_normalization_111[0][0]’]\nactivation_112 (Activation) (None, 10, 10, 64) 0 [‘batch_normalization_112[0][0]’]\nmixed1 (Concatenate) (None, 10, 10, 288) 0 [‘activation_106[0][0]’, ‘activation_108[0][0]’, ‘activation_111[0][0]’, ‘activation_112[0][0]’]\nconv2d_116 (Conv2D) (None, 10, 10, 64) 18432 [‘mixed1[0][0]’]\nbatch_normalization_116 (Batch (None, 10, 10, 64) 192 [‘conv2d_116[0][0]’] Normalization)\nactivation_116 (Activation) (None, 10, 10, 64) 0 [‘batch_normalization_116[0][0]’]\nconv2d_114 (Conv2D) (None, 10, 10, 48) 13824 [‘mixed1[0][0]’]\nconv2d_117 (Conv2D) (None, 10, 10, 96) 55296 [‘activation_116[0][0]’]\nbatch_normalization_114 (Batch (None, 10, 10, 48) 144 [‘conv2d_114[0][0]’] Normalization)\nbatch_normalization_117 (Batch (None, 10, 10, 96) 288 [‘conv2d_117[0][0]’] Normalization)\nactivation_114 (Activation) (None, 10, 10, 48) 0 [‘batch_normalization_114[0][0]’]\nactivation_117 (Activation) (None, 10, 10, 96) 0 [‘batch_normalization_117[0][0]’]\naverage_pooling2d_11 (AverageP (None, 10, 10, 288) 0 [‘mixed1[0][0]’] ooling2D)\nconv2d_113 (Conv2D) (None, 10, 10, 64) 18432 [‘mixed1[0][0]’]\nconv2d_115 (Conv2D) (None, 10, 10, 64) 76800 [‘activation_114[0][0]’]\nconv2d_118 (Conv2D) (None, 10, 10, 96) 82944 [‘activation_117[0][0]’]\nconv2d_119 (Conv2D) (None, 10, 10, 64) 18432 [‘average_pooling2d_11[0][0]’]\nbatch_normalization_113 (Batch (None, 10, 10, 64) 192 [‘conv2d_113[0][0]’] Normalization)\nbatch_normalization_115 (Batch (None, 10, 10, 64) 192 [‘conv2d_115[0][0]’] Normalization)\nbatch_normalization_118 (Batch (None, 10, 10, 96) 288 [‘conv2d_118[0][0]’] Normalization)\nbatch_normalization_119 (Batch (None, 10, 10, 64) 192 [‘conv2d_119[0][0]’] Normalization)\nactivation_113 (Activation) (None, 10, 10, 64) 0 [‘batch_normalization_113[0][0]’]\nactivation_115 (Activation) (None, 10, 10, 64) 0 [‘batch_normalization_115[0][0]’]\nactivation_118 (Activation) (None, 10, 10, 96) 0 [‘batch_normalization_118[0][0]’]\nactivation_119 (Activation) (None, 10, 10, 64) 0 [‘batch_normalization_119[0][0]’]\nmixed2 (Concatenate) (None, 10, 10, 288) 0 [‘activation_113[0][0]’, ‘activation_115[0][0]’, ‘activation_118[0][0]’, ‘activation_119[0][0]’]\nconv2d_121 (Conv2D) (None, 10, 10, 64) 18432 [‘mixed2[0][0]’]\nbatch_normalization_121 (Batch (None, 10, 10, 64) 192 [‘conv2d_121[0][0]’] Normalization)\nactivation_121 (Activation) (None, 10, 10, 64) 0 [‘batch_normalization_121[0][0]’]\nconv2d_122 (Conv2D) (None, 10, 10, 96) 55296 [‘activation_121[0][0]’]\nbatch_normalization_122 (Batch (None, 10, 10, 96) 288 [‘conv2d_122[0][0]’] Normalization)\nactivation_122 (Activation) (None, 10, 10, 96) 0 [‘batch_normalization_122[0][0]’]\nconv2d_120 (Conv2D) (None, 4, 4, 384) 995328 [‘mixed2[0][0]’]\nconv2d_123 (Conv2D) (None, 4, 4, 96) 82944 [‘activation_122[0][0]’]\nbatch_normalization_120 (Batch (None, 4, 4, 384) 1152 [‘conv2d_120[0][0]’] Normalization)\nbatch_normalization_123 (Batch (None, 4, 4, 96) 288 [‘conv2d_123[0][0]’] Normalization)\nactivation_120 (Activation) (None, 4, 4, 384) 0 [‘batch_normalization_120[0][0]’]\nactivation_123 (Activation) (None, 4, 4, 96) 0 [‘batch_normalization_123[0][0]’]\nmax_pooling2d_6 (MaxPooling2D) (None, 4, 4, 288) 0 [‘mixed2[0][0]’]\nmixed3 (Concatenate) (None, 4, 4, 768) 0 [‘activation_120[0][0]’, ‘activation_123[0][0]’, ‘max_pooling2d_6[0][0]’]\nconv2d_128 (Conv2D) (None, 4, 4, 128) 98304 [‘mixed3[0][0]’]\nbatch_normalization_128 (Batch (None, 4, 4, 128) 384 [‘conv2d_128[0][0]’] Normalization)\nactivation_128 (Activation) (None, 4, 4, 128) 0 [‘batch_normalization_128[0][0]’]\nconv2d_129 (Conv2D) (None, 4, 4, 128) 114688 [‘activation_128[0][0]’]\nbatch_normalization_129 (Batch (None, 4, 4, 128) 384 [‘conv2d_129[0][0]’] Normalization)\nactivation_129 (Activation) (None, 4, 4, 128) 0 [‘batch_normalization_129[0][0]’]\nconv2d_125 (Conv2D) (None, 4, 4, 128) 98304 [‘mixed3[0][0]’]\nconv2d_130 (Conv2D) (None, 4, 4, 128) 114688 [‘activation_129[0][0]’]\nbatch_normalization_125 (Batch (None, 4, 4, 128) 384 [‘conv2d_125[0][0]’] Normalization)\nbatch_normalization_130 (Batch (None, 4, 4, 128) 384 [‘conv2d_130[0][0]’] Normalization)\nactivation_125 (Activation) (None, 4, 4, 128) 0 [‘batch_normalization_125[0][0]’]\nactivation_130 (Activation) (None, 4, 4, 128) 0 [‘batch_normalization_130[0][0]’]\nconv2d_126 (Conv2D) (None, 4, 4, 128) 114688 [‘activation_125[0][0]’]\nconv2d_131 (Conv2D) (None, 4, 4, 128) 114688 [‘activation_130[0][0]’]\nbatch_normalization_126 (Batch (None, 4, 4, 128) 384 [‘conv2d_126[0][0]’] Normalization)\nbatch_normalization_131 (Batch (None, 4, 4, 128) 384 [‘conv2d_131[0][0]’] Normalization)\nactivation_126 (Activation) (None, 4, 4, 128) 0 [‘batch_normalization_126[0][0]’]\nactivation_131 (Activation) (None, 4, 4, 128) 0 [‘batch_normalization_131[0][0]’]\naverage_pooling2d_12 (AverageP (None, 4, 4, 768) 0 [‘mixed3[0][0]’] ooling2D)\nconv2d_124 (Conv2D) (None, 4, 4, 192) 147456 [‘mixed3[0][0]’]\nconv2d_127 (Conv2D) (None, 4, 4, 192) 172032 [‘activation_126[0][0]’]\nconv2d_132 (Conv2D) (None, 4, 4, 192) 172032 [‘activation_131[0][0]’]\nconv2d_133 (Conv2D) (None, 4, 4, 192) 147456 [‘average_pooling2d_12[0][0]’]\nbatch_normalization_124 (Batch (None, 4, 4, 192) 576 [‘conv2d_124[0][0]’] Normalization)\nbatch_normalization_127 (Batch (None, 4, 4, 192) 576 [‘conv2d_127[0][0]’] Normalization)\nbatch_normalization_132 (Batch (None, 4, 4, 192) 576 [‘conv2d_132[0][0]’] Normalization)\nbatch_normalization_133 (Batch (None, 4, 4, 192) 576 [‘conv2d_133[0][0]’] Normalization)\nactivation_124 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_124[0][0]’]\nactivation_127 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_127[0][0]’]\nactivation_132 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_132[0][0]’]\nactivation_133 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_133[0][0]’]\nmixed4 (Concatenate) (None, 4, 4, 768) 0 [‘activation_124[0][0]’, ‘activation_127[0][0]’, ‘activation_132[0][0]’, ‘activation_133[0][0]’]\nconv2d_138 (Conv2D) (None, 4, 4, 160) 122880 [‘mixed4[0][0]’]\nbatch_normalization_138 (Batch (None, 4, 4, 160) 480 [‘conv2d_138[0][0]’] Normalization)\nactivation_138 (Activation) (None, 4, 4, 160) 0 [‘batch_normalization_138[0][0]’]\nconv2d_139 (Conv2D) (None, 4, 4, 160) 179200 [‘activation_138[0][0]’]\nbatch_normalization_139 (Batch (None, 4, 4, 160) 480 [‘conv2d_139[0][0]’] Normalization)\nactivation_139 (Activation) (None, 4, 4, 160) 0 [‘batch_normalization_139[0][0]’]\nconv2d_135 (Conv2D) (None, 4, 4, 160) 122880 [‘mixed4[0][0]’]\nconv2d_140 (Conv2D) (None, 4, 4, 160) 179200 [‘activation_139[0][0]’]\nbatch_normalization_135 (Batch (None, 4, 4, 160) 480 [‘conv2d_135[0][0]’] Normalization)\nbatch_normalization_140 (Batch (None, 4, 4, 160) 480 [‘conv2d_140[0][0]’] Normalization)\nactivation_135 (Activation) (None, 4, 4, 160) 0 [‘batch_normalization_135[0][0]’]\nactivation_140 (Activation) (None, 4, 4, 160) 0 [‘batch_normalization_140[0][0]’]\nconv2d_136 (Conv2D) (None, 4, 4, 160) 179200 [‘activation_135[0][0]’]\nconv2d_141 (Conv2D) (None, 4, 4, 160) 179200 [‘activation_140[0][0]’]\nbatch_normalization_136 (Batch (None, 4, 4, 160) 480 [‘conv2d_136[0][0]’] Normalization)\nbatch_normalization_141 (Batch (None, 4, 4, 160) 480 [‘conv2d_141[0][0]’] Normalization)\nactivation_136 (Activation) (None, 4, 4, 160) 0 [‘batch_normalization_136[0][0]’]\nactivation_141 (Activation) (None, 4, 4, 160) 0 [‘batch_normalization_141[0][0]’]\naverage_pooling2d_13 (AverageP (None, 4, 4, 768) 0 [‘mixed4[0][0]’] ooling2D)\nconv2d_134 (Conv2D) (None, 4, 4, 192) 147456 [‘mixed4[0][0]’]\nconv2d_137 (Conv2D) (None, 4, 4, 192) 215040 [‘activation_136[0][0]’]\nconv2d_142 (Conv2D) (None, 4, 4, 192) 215040 [‘activation_141[0][0]’]\nconv2d_143 (Conv2D) (None, 4, 4, 192) 147456 [‘average_pooling2d_13[0][0]’]\nbatch_normalization_134 (Batch (None, 4, 4, 192) 576 [‘conv2d_134[0][0]’] Normalization)\nbatch_normalization_137 (Batch (None, 4, 4, 192) 576 [‘conv2d_137[0][0]’] Normalization)\nbatch_normalization_142 (Batch (None, 4, 4, 192) 576 [‘conv2d_142[0][0]’] Normalization)\nbatch_normalization_143 (Batch (None, 4, 4, 192) 576 [‘conv2d_143[0][0]’] Normalization)\nactivation_134 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_134[0][0]’]\nactivation_137 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_137[0][0]’]\nactivation_142 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_142[0][0]’]\nactivation_143 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_143[0][0]’]\nmixed5 (Concatenate) (None, 4, 4, 768) 0 [‘activation_134[0][0]’, ‘activation_137[0][0]’, ‘activation_142[0][0]’, ‘activation_143[0][0]’]\nconv2d_148 (Conv2D) (None, 4, 4, 160) 122880 [‘mixed5[0][0]’]\nbatch_normalization_148 (Batch (None, 4, 4, 160) 480 [‘conv2d_148[0][0]’] Normalization)\nactivation_148 (Activation) (None, 4, 4, 160) 0 [‘batch_normalization_148[0][0]’]\nconv2d_149 (Conv2D) (None, 4, 4, 160) 179200 [‘activation_148[0][0]’]\nbatch_normalization_149 (Batch (None, 4, 4, 160) 480 [‘conv2d_149[0][0]’] Normalization)\nactivation_149 (Activation) (None, 4, 4, 160) 0 [‘batch_normalization_149[0][0]’]\nconv2d_145 (Conv2D) (None, 4, 4, 160) 122880 [‘mixed5[0][0]’]\nconv2d_150 (Conv2D) (None, 4, 4, 160) 179200 [‘activation_149[0][0]’]\nbatch_normalization_145 (Batch (None, 4, 4, 160) 480 [‘conv2d_145[0][0]’] Normalization)\nbatch_normalization_150 (Batch (None, 4, 4, 160) 480 [‘conv2d_150[0][0]’] Normalization)\nactivation_145 (Activation) (None, 4, 4, 160) 0 [‘batch_normalization_145[0][0]’]\nactivation_150 (Activation) (None, 4, 4, 160) 0 [‘batch_normalization_150[0][0]’]\nconv2d_146 (Conv2D) (None, 4, 4, 160) 179200 [‘activation_145[0][0]’]\nconv2d_151 (Conv2D) (None, 4, 4, 160) 179200 [‘activation_150[0][0]’]\nbatch_normalization_146 (Batch (None, 4, 4, 160) 480 [‘conv2d_146[0][0]’] Normalization)\nbatch_normalization_151 (Batch (None, 4, 4, 160) 480 [‘conv2d_151[0][0]’] Normalization)\nactivation_146 (Activation) (None, 4, 4, 160) 0 [‘batch_normalization_146[0][0]’]\nactivation_151 (Activation) (None, 4, 4, 160) 0 [‘batch_normalization_151[0][0]’]\naverage_pooling2d_14 (AverageP (None, 4, 4, 768) 0 [‘mixed5[0][0]’] ooling2D)\nconv2d_144 (Conv2D) (None, 4, 4, 192) 147456 [‘mixed5[0][0]’]\nconv2d_147 (Conv2D) (None, 4, 4, 192) 215040 [‘activation_146[0][0]’]\nconv2d_152 (Conv2D) (None, 4, 4, 192) 215040 [‘activation_151[0][0]’]\nconv2d_153 (Conv2D) (None, 4, 4, 192) 147456 [‘average_pooling2d_14[0][0]’]\nbatch_normalization_144 (Batch (None, 4, 4, 192) 576 [‘conv2d_144[0][0]’] Normalization)\nbatch_normalization_147 (Batch (None, 4, 4, 192) 576 [‘conv2d_147[0][0]’] Normalization)\nbatch_normalization_152 (Batch (None, 4, 4, 192) 576 [‘conv2d_152[0][0]’] Normalization)\nbatch_normalization_153 (Batch (None, 4, 4, 192) 576 [‘conv2d_153[0][0]’] Normalization)\nactivation_144 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_144[0][0]’]\nactivation_147 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_147[0][0]’]\nactivation_152 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_152[0][0]’]\nactivation_153 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_153[0][0]’]\nmixed6 (Concatenate) (None, 4, 4, 768) 0 [‘activation_144[0][0]’, ‘activation_147[0][0]’, ‘activation_152[0][0]’, ‘activation_153[0][0]’]\nconv2d_158 (Conv2D) (None, 4, 4, 192) 147456 [‘mixed6[0][0]’]\nbatch_normalization_158 (Batch (None, 4, 4, 192) 576 [‘conv2d_158[0][0]’] Normalization)\nactivation_158 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_158[0][0]’]\nconv2d_159 (Conv2D) (None, 4, 4, 192) 258048 [‘activation_158[0][0]’]\nbatch_normalization_159 (Batch (None, 4, 4, 192) 576 [‘conv2d_159[0][0]’] Normalization)\nactivation_159 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_159[0][0]’]\nconv2d_155 (Conv2D) (None, 4, 4, 192) 147456 [‘mixed6[0][0]’]\nconv2d_160 (Conv2D) (None, 4, 4, 192) 258048 [‘activation_159[0][0]’]\nbatch_normalization_155 (Batch (None, 4, 4, 192) 576 [‘conv2d_155[0][0]’] Normalization)\nbatch_normalization_160 (Batch (None, 4, 4, 192) 576 [‘conv2d_160[0][0]’] Normalization)\nactivation_155 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_155[0][0]’]\nactivation_160 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_160[0][0]’]\nconv2d_156 (Conv2D) (None, 4, 4, 192) 258048 [‘activation_155[0][0]’]\nconv2d_161 (Conv2D) (None, 4, 4, 192) 258048 [‘activation_160[0][0]’]\nbatch_normalization_156 (Batch (None, 4, 4, 192) 576 [‘conv2d_156[0][0]’] Normalization)\nbatch_normalization_161 (Batch (None, 4, 4, 192) 576 [‘conv2d_161[0][0]’] Normalization)\nactivation_156 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_156[0][0]’]\nactivation_161 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_161[0][0]’]\naverage_pooling2d_15 (AverageP (None, 4, 4, 768) 0 [‘mixed6[0][0]’] ooling2D)\nconv2d_154 (Conv2D) (None, 4, 4, 192) 147456 [‘mixed6[0][0]’]\nconv2d_157 (Conv2D) (None, 4, 4, 192) 258048 [‘activation_156[0][0]’]\nconv2d_162 (Conv2D) (None, 4, 4, 192) 258048 [‘activation_161[0][0]’]\nconv2d_163 (Conv2D) (None, 4, 4, 192) 147456 [‘average_pooling2d_15[0][0]’]\nbatch_normalization_154 (Batch (None, 4, 4, 192) 576 [‘conv2d_154[0][0]’] Normalization)\nbatch_normalization_157 (Batch (None, 4, 4, 192) 576 [‘conv2d_157[0][0]’] Normalization)\nbatch_normalization_162 (Batch (None, 4, 4, 192) 576 [‘conv2d_162[0][0]’] Normalization)\nbatch_normalization_163 (Batch (None, 4, 4, 192) 576 [‘conv2d_163[0][0]’] Normalization)\nactivation_154 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_154[0][0]’]\nactivation_157 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_157[0][0]’]\nactivation_162 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_162[0][0]’]\nactivation_163 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_163[0][0]’]\nmixed7 (Concatenate) (None, 4, 4, 768) 0 [‘activation_154[0][0]’, ‘activation_157[0][0]’, ‘activation_162[0][0]’, ‘activation_163[0][0]’]\nconv2d_166 (Conv2D) (None, 4, 4, 192) 147456 [‘mixed7[0][0]’]\nbatch_normalization_166 (Batch (None, 4, 4, 192) 576 [‘conv2d_166[0][0]’] Normalization)\nactivation_166 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_166[0][0]’]\nconv2d_167 (Conv2D) (None, 4, 4, 192) 258048 [‘activation_166[0][0]’]\nbatch_normalization_167 (Batch (None, 4, 4, 192) 576 [‘conv2d_167[0][0]’] Normalization)\nactivation_167 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_167[0][0]’]\nconv2d_164 (Conv2D) (None, 4, 4, 192) 147456 [‘mixed7[0][0]’]\nconv2d_168 (Conv2D) (None, 4, 4, 192) 258048 [‘activation_167[0][0]’]\nbatch_normalization_164 (Batch (None, 4, 4, 192) 576 [‘conv2d_164[0][0]’] Normalization)\nbatch_normalization_168 (Batch (None, 4, 4, 192) 576 [‘conv2d_168[0][0]’] Normalization)\nactivation_164 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_164[0][0]’]\nactivation_168 (Activation) (None, 4, 4, 192) 0 [‘batch_normalization_168[0][0]’]\nconv2d_165 (Conv2D) (None, 1, 1, 320) 552960 [‘activation_164[0][0]’]\nconv2d_169 (Conv2D) (None, 1, 1, 192) 331776 [‘activation_168[0][0]’]\nbatch_normalization_165 (Batch (None, 1, 1, 320) 960 [‘conv2d_165[0][0]’] Normalization)\nbatch_normalization_169 (Batch (None, 1, 1, 192) 576 [‘conv2d_169[0][0]’] Normalization)\nactivation_165 (Activation) (None, 1, 1, 320) 0 [‘batch_normalization_165[0][0]’]\nactivation_169 (Activation) (None, 1, 1, 192) 0 [‘batch_normalization_169[0][0]’]\nmax_pooling2d_7 (MaxPooling2D) (None, 1, 1, 768) 0 [‘mixed7[0][0]’]\nmixed8 (Concatenate) (None, 1, 1, 1280) 0 [‘activation_165[0][0]’, ‘activation_169[0][0]’, ‘max_pooling2d_7[0][0]’]\nconv2d_174 (Conv2D) (None, 1, 1, 448) 573440 [‘mixed8[0][0]’]\nbatch_normalization_174 (Batch (None, 1, 1, 448) 1344 [‘conv2d_174[0][0]’] Normalization)\nactivation_174 (Activation) (None, 1, 1, 448) 0 [‘batch_normalization_174[0][0]’]\nconv2d_171 (Conv2D) (None, 1, 1, 384) 491520 [‘mixed8[0][0]’]\nconv2d_175 (Conv2D) (None, 1, 1, 384) 1548288 [‘activation_174[0][0]’]\nbatch_normalization_171 (Batch (None, 1, 1, 384) 1152 [‘conv2d_171[0][0]’] Normalization)\nbatch_normalization_175 (Batch (None, 1, 1, 384) 1152 [‘conv2d_175[0][0]’] Normalization)\nactivation_171 (Activation) (None, 1, 1, 384) 0 [‘batch_normalization_171[0][0]’]\nactivation_175 (Activation) (None, 1, 1, 384) 0 [‘batch_normalization_175[0][0]’]\nconv2d_172 (Conv2D) (None, 1, 1, 384) 442368 [‘activation_171[0][0]’]\nconv2d_173 (Conv2D) (None, 1, 1, 384) 442368 [‘activation_171[0][0]’]\nconv2d_176 (Conv2D) (None, 1, 1, 384) 442368 [‘activation_175[0][0]’]\nconv2d_177 (Conv2D) (None, 1, 1, 384) 442368 [‘activation_175[0][0]’]\naverage_pooling2d_16 (AverageP (None, 1, 1, 1280) 0 [‘mixed8[0][0]’] ooling2D)\nconv2d_170 (Conv2D) (None, 1, 1, 320) 409600 [‘mixed8[0][0]’]\nbatch_normalization_172 (Batch (None, 1, 1, 384) 1152 [‘conv2d_172[0][0]’] Normalization)\nbatch_normalization_173 (Batch (None, 1, 1, 384) 1152 [‘conv2d_173[0][0]’] Normalization)\nbatch_normalization_176 (Batch (None, 1, 1, 384) 1152 [‘conv2d_176[0][0]’] Normalization)\nbatch_normalization_177 (Batch (None, 1, 1, 384) 1152 [‘conv2d_177[0][0]’] Normalization)\nconv2d_178 (Conv2D) (None, 1, 1, 192) 245760 [‘average_pooling2d_16[0][0]’]\nbatch_normalization_170 (Batch (None, 1, 1, 320) 960 [‘conv2d_170[0][0]’] Normalization)\nactivation_172 (Activation) (None, 1, 1, 384) 0 [‘batch_normalization_172[0][0]’]\nactivation_173 (Activation) (None, 1, 1, 384) 0 [‘batch_normalization_173[0][0]’]\nactivation_176 (Activation) (None, 1, 1, 384) 0 [‘batch_normalization_176[0][0]’]\nactivation_177 (Activation) (None, 1, 1, 384) 0 [‘batch_normalization_177[0][0]’]\nbatch_normalization_178 (Batch (None, 1, 1, 192) 576 [‘conv2d_178[0][0]’] Normalization)\nactivation_170 (Activation) (None, 1, 1, 320) 0 [‘batch_normalization_170[0][0]’]\nmixed9_0 (Concatenate) (None, 1, 1, 768) 0 [‘activation_172[0][0]’, ‘activation_173[0][0]’]\nconcatenate_2 (Concatenate) (None, 1, 1, 768) 0 [‘activation_176[0][0]’, ‘activation_177[0][0]’]\nactivation_178 (Activation) (None, 1, 1, 192) 0 [‘batch_normalization_178[0][0]’]\nmixed9 (Concatenate) (None, 1, 1, 2048) 0 [‘activation_170[0][0]’, ‘mixed9_0[0][0]’, ‘concatenate_2[0][0]’, ‘activation_178[0][0]’]\nconv2d_183 (Conv2D) (None, 1, 1, 448) 917504 [‘mixed9[0][0]’]\nbatch_normalization_183 (Batch (None, 1, 1, 448) 1344 [‘conv2d_183[0][0]’] Normalization)\nactivation_183 (Activation) (None, 1, 1, 448) 0 [‘batch_normalization_183[0][0]’]\nconv2d_180 (Conv2D) (None, 1, 1, 384) 786432 [‘mixed9[0][0]’]\nconv2d_184 (Conv2D) (None, 1, 1, 384) 1548288 [‘activation_183[0][0]’]\nbatch_normalization_180 (Batch (None, 1, 1, 384) 1152 [‘conv2d_180[0][0]’] Normalization)\nbatch_normalization_184 (Batch (None, 1, 1, 384) 1152 [‘conv2d_184[0][0]’] Normalization)\nactivation_180 (Activation) (None, 1, 1, 384) 0 [‘batch_normalization_180[0][0]’]\nactivation_184 (Activation) (None, 1, 1, 384) 0 [‘batch_normalization_184[0][0]’]\nconv2d_181 (Conv2D) (None, 1, 1, 384) 442368 [‘activation_180[0][0]’]\nconv2d_182 (Conv2D) (None, 1, 1, 384) 442368 [‘activation_180[0][0]’]\nconv2d_185 (Conv2D) (None, 1, 1, 384) 442368 [‘activation_184[0][0]’]\nconv2d_186 (Conv2D) (None, 1, 1, 384) 442368 [‘activation_184[0][0]’]\naverage_pooling2d_17 (AverageP (None, 1, 1, 2048) 0 [‘mixed9[0][0]’] ooling2D)\nconv2d_179 (Conv2D) (None, 1, 1, 320) 655360 [‘mixed9[0][0]’]\nbatch_normalization_181 (Batch (None, 1, 1, 384) 1152 [‘conv2d_181[0][0]’] Normalization)\nbatch_normalization_182 (Batch (None, 1, 1, 384) 1152 [‘conv2d_182[0][0]’] Normalization)\nbatch_normalization_185 (Batch (None, 1, 1, 384) 1152 [‘conv2d_185[0][0]’] Normalization)\nbatch_normalization_186 (Batch (None, 1, 1, 384) 1152 [‘conv2d_186[0][0]’] Normalization)\nconv2d_187 (Conv2D) (None, 1, 1, 192) 393216 [‘average_pooling2d_17[0][0]’]\nbatch_normalization_179 (Batch (None, 1, 1, 320) 960 [‘conv2d_179[0][0]’] Normalization)\nactivation_181 (Activation) (None, 1, 1, 384) 0 [‘batch_normalization_181[0][0]’]\nactivation_182 (Activation) (None, 1, 1, 384) 0 [‘batch_normalization_182[0][0]’]\nactivation_185 (Activation) (None, 1, 1, 384) 0 [‘batch_normalization_185[0][0]’]\nactivation_186 (Activation) (None, 1, 1, 384) 0 [‘batch_normalization_186[0][0]’]\nbatch_normalization_187 (Batch (None, 1, 1, 192) 576 [‘conv2d_187[0][0]’] Normalization)\nactivation_179 (Activation) (None, 1, 1, 320) 0 [‘batch_normalization_179[0][0]’]\nmixed9_1 (Concatenate) (None, 1, 1, 768) 0 [‘activation_181[0][0]’, ‘activation_182[0][0]’]\nconcatenate_3 (Concatenate) (None, 1, 1, 768) 0 [‘activation_185[0][0]’, ‘activation_186[0][0]’]\nactivation_187 (Activation) (None, 1, 1, 192) 0 [‘batch_normalization_187[0][0]’]\nmixed10 (Concatenate) (None, 1, 1, 2048) 0 [‘activation_179[0][0]’, ‘mixed9_1[0][0]’, ‘concatenate_3[0][0]’, ‘activation_187[0][0]’]\nglobal_average_pooling2d_1 (Gl (None, 2048) 0 [‘mixed10[0][0]’] obalAveragePooling2D)\ndense_2 (Dense) (None, 1024) 2098176 [‘global_average_pooling2d_1[0][0 ]’]\ndense_3 (Dense) (None, 1) 1025 [‘dense_2[0][0]’]\n================================================================================================== Total params: 23,901,985 Trainable params: 2,099,201 Non-trainable params: 21,802,784 __________________________________________________________________________________________________\nsaveLoadpath = '/content/drive/MyDrive/RTL_Lubricity/Machine_Vision/Hucklebery/6_8_2023/Test_1/'\n\nImported_model.save(saveLoadpath + 'Test_1_A.h5')\nsaveLoadpath = '/content/drive/MyDrive/RTL_Lubricity/Machine_Vision/Hucklebery/6_8_2023/Test_1/'\n\nfrom keras.models import load_model\nImported_model = load_model(saveLoadpath + 'Test_1_A.h5')"
  },
  {
    "objectID": "posts/Reinforcement Learning/index.html",
    "href": "posts/Reinforcement Learning/index.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "RL for Drone Navigation\nIn this project, an RL environment will be setup to train an object to start from a starting point and pan towards the destination in 2D space\n!pip install stable_baselines3\n!pip install gymnasium\n!pip install tensorboard\n!pip install ipywidgets\nfrom google.colab import drive\ndrive.mount('/content/drive')\nimport stable_baselines3\nfrom gymnasium import spaces, Env\n\nimport numpy as np\n\nGRID_SIZE  = np.array([10.0,    10.0], dtype=np.float32)\nMAGN_RANGE = np.array([0.0,      1.0], dtype=np.float32)\nHEAD_RANGE = np.array([0.0,2 * np.pi], dtype=np.float32)\n\nd_magn = 0.1\n\nTARGET_POSITION      = np.array([7.0, 5.0], dtype=np.float32)\nREWARD_CIRCLE_RADIUS = 0.5 #units\nMAX_EPISODE_STEPS    = 512\nMAX_RW_CIRCLE_STEPS  = 16\n\nclass Env2D(Env):\n\n    def __init__(self):\n\n        # Action space: Magnitude and Heading\n        self.action_space = spaces.Box(low  = np.array([MAGN_RANGE[0], HEAD_RANGE[0]]),\n                                       high = np.array([MAGN_RANGE[1], HEAD_RANGE[1]]),\n                                       dtype=np.float32)\n\n        # Observation space: Position, Target Position, Heading\n\n        low_obs  = np.array([#Minimum values these can take\n                                              0,                  0,\n                             TARGET_POSITION[0], TARGET_POSITION[1],\n                             HEAD_RANGE[0],\n                             MAGN_RANGE[0]\n        ], dtype=np.float32)\n\n        high_obs = np.array([#Maximum values these can take\n                             GRID_SIZE[0]      ,        GRID_SIZE[1],\n                             TARGET_POSITION[0],  TARGET_POSITION[1],\n                             HEAD_RANGE[1],\n                             MAGN_RANGE[1]\n        ], dtype=np.float32)\n\n        self.observation_space = spaces.Box(low = low_obs, high = high_obs, dtype = np.float32)\n\n        # Call reset to initialize the agent's position and heading\n        self.reset()\n\n    def reset(self, seed=None):\n\n        if seed:\n            np.random.seed(seed)\n\n        # Initialize agent positioned randomly within the grid, with random heading and 0 magnitude\n        self.position  = np.array([np.random.uniform(0, GRID_SIZE[0]), np.random.uniform(0, GRID_SIZE[1])], dtype=np.float32)\n        self.heading   = np.random.uniform(HEAD_RANGE[0], HEAD_RANGE[1])\n        self.magnitude = 0.0\n\n        # Initialize reward buffer for the last MAX_RW_CIRCLE steps\n        self.reward_buffer = np.full((MAX_RW_CIRCLE_STEPS,), -np.inf)\n\n        #Episode length counter\n        self.current_step = 0\n\n        return self._get_obs(), {}\n\n    def _get_obs(self):\n        # Return observation space\n        return np.array([\n            self.position[0]  , self.position[1],\n            TARGET_POSITION[0], TARGET_POSITION[1],\n            self.heading,\n            self.magnitude\n        ], dtype=np.float32)\n\n    def step(self, action):\n\n        # Update and clip heading and magnitude based on action\n        self.magnitude = np.clip(action[0], MAGN_RANGE[0], MAGN_RANGE[1])\n        self.heading = (self.heading + action[1]) % (2 * np.pi)\n\n        # Move agent according to given heading\n        dx = self.magnitude * np.cos(self.heading) * d_magn\n        dy = self.magnitude * np.sin(self.heading) * d_magn\n        self.position = np.array([self.position[0] + dx, self.position[1] + dy], dtype=np.float32)\n        self.position = np.clip(self.position, [0, 0], GRID_SIZE)\n\n        # Calculate reward\n        distance_to_target = np.linalg.norm(self.position - TARGET_POSITION)\n        reward = -distance_to_target\n\n        # Update reward buffer\n        self.reward_buffer[:-1] = self.reward_buffer[1:]\n        self.reward_buffer[-1] = reward\n\n        #increment step counter\n        self.current_step += 1\n\n        # Calculate 'truncated'\n        truncated = self.current_step &gt;= MAX_EPISODE_STEPS\n\n        # Calculate 'terminated'\n        terminated = np.all(self.reward_buffer &gt;= -REWARD_CIRCLE_RADIUS)\n\n        return self._get_obs(), reward, bool(terminated), bool(truncated), {}\n\n    def render(self, mode='human'):\n        pass\n\n    def close(self):\n        pass\nIn order to make it faster it was trained for 2500000 steps but actually it needs 25000000 steps to successfully reach the target\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_checker import check_env\n\nenv = Env2D()\ncheck_env(env)\n\n#log\nlog_dir = \"logs/\" #tensorboard --logdir logs\n\n# Initialize the agent\ntrain = True #true\nif train:\n    model = PPO(\"MlpPolicy\", env, verbose=0, tensorboard_log=log_dir)\n    model.learn(total_timesteps=2500000, progress_bar=True)\nenv = Env2D()\ncheck_env(env)\n\nframes_all = []\nfor _ in range(10):\n\n    #Run one collect\n    obs, _info = env.reset()\n    frames = []\n\n    # Run an episode\n    terminated = truncated = False\n    while not (terminated or truncated):\n\n        #compute action\n        action, state_ = model.predict(obs, deterministic=True)\n\n        #store obs-state-action\n        frames.append((obs, action))\n\n        #environment takes action\n        obs, reward, terminated, truncated, _info = env.step(action)\n\n    frames_all.append(frames)\n\nenv.close()\n\nFor each iteration, the motion of object can be visualized\nimport pygame\nimport math\n\n# Initialize pygame\npygame.init()\n\n# Set the dimensions of the window\nWIDTH, HEIGHT = 800, 800\nwin = pygame.display.set_mode((WIDTH, HEIGHT))\npygame.display.set_caption(\"Simple Animation\")\n\n# Colors\nWHITE = (255, 255, 255)\nRED = (255, 0, 0)\nBLUE = (0, 0, 255)\nGREEN = (0, 255, 0)\n\n# Scale factor to fit (10,10) position within the window size\nSCALE = 80 # 800/10 = 80\n\ndef draw_position_and_heading(position, target, heading, thrust):\n\n    # Draw the position\n    pygame.draw.circle(win, RED, (int(position[0]*SCALE), int(position[1]*SCALE)), 10)\n\n    # Draw the target\n    pygame.draw.circle(win, GREEN, (int(target[0]*SCALE), int(target[1]*SCALE)), 5)\n\n    # Draw the heading\n    end_x = int(position[0]*SCALE + thrust * 50 * math.cos(heading))\n    end_y = int(position[1]*SCALE + thrust * 50 * math.sin(heading))\n    pygame.draw.line(win, BLUE, (int(position[0]*SCALE), int(position[1]*SCALE)), (end_x, end_y), 2)\n\ndef main(frames):\n    clock = pygame.time.Clock()\n    running = True\n    frames_idx = 0\n\n    while running:\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                running = False\n\n        win.fill(WHITE)  # Fill the screen with white\n        for frames_set in frames_all:\n            if frames_idx &lt; len(frames_set):\n                frame = frames_set[frames_idx]\n                position = frame[0][0:2]\n                target = frame[0][2:4]\n                heading = frame[0][4]\n                thrust = frame[1][0]\n                draw_position_and_heading(position, target, heading, thrust)\n\n        frames_idx += 1\n        pygame.time.wait(100)\n        pygame.display.update()\n\n    pygame.quit()\n\nmain(frames_all)"
  }
]
[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MortezaML",
    "section": "",
    "text": "This study develops an effective system for in-motion and autonomous identification of crossties and ballast, toward implementing non-contact sensors that can evaluate the condition of railroad structures. An array of sensors that include distance LIDAR sensors, magnetic sensors, and cameras are employed to test a prototype system in motion, onboard a remotely controlled track cart that can travel at speeds of up to 10 mph. The tests are performed on revenue-service tracks and the measurements are used to develop autonomous post-processing approaches that can be readily adopted by the railroads. Two distinct techniques for the LIDAR sensors are explored. Next, a machine learning model is developed to achieve the task with potentially more accuracy. To this end, three machine learning models, using three types of inputs, are developed to identify the optimal model. The DecisionTree algorithm coupled with the standard deviation of the difference between two distance sensors proved to be the most effective.\nfrom google.colab import drive\ndrive.mount('/content/drive')\nDefine a custom function for plotting\ndef interactive_plot_scatter(df, x_axis, y_axis):\n  import plotly.express as px\n\n  fig = px.scatter(df, x_axis, y_axis)\n  return fig.show()\n\ndef interactive_plot_line(df, x_axis, y_axis):\n  import plotly.express as px\n\n  fig = px.line(df, x_axis, y_axis)\n  return fig.show()\n\ndef tach_cleaning(df):\n  tach = np.zeros(df['Tach'].shape[0])\n  for i in range(1, df['Tach'].shape[0]):\n    if df.iloc[i]['Tach'] &gt; 2.2:\n      tach[i] = 5\n  df['Tach_p'] = tach\n  return df\n\ndef position_string(df: dict) -&gt; str:\n  df['Pos'] = 0.000000\n  pos = df['Pos']\n  c = 0;\n  delta = float((1/36) * 7.25 * np.pi / 12);\n  for i in range(1, pos.shape[0]):\n    if (np.abs(df.iloc[i]['Tach']-df.iloc[i-1]['Tach'])&gt;1):\n      c = c + 1\n    pos[i] = delta * c\n  return pos\n\ndef remove_Keyence_dropout(df):\n  df = df[df['left_disp']&lt;1.15]\n  df = df[df['right_disp']&lt;0.925]\n  return df\n\ndef remove_outliers(df):\n  Q1 = np.percentile(df['right_disp'], 25,\n                   interpolation = 'midpoint')\n\n  Q3 = np.percentile(df['right_disp'], 75,\n                   interpolation = 'midpoint')\n  IQR = Q3 - Q1\n\n  up = Q3+1.5*IQR\n  low = Q1-1.5*IQR\n\n  df = df[df['right_disp']&lt;up]\n  df = df[df['right_disp']&gt;low]\n\n  Q1 = np.percentile(df['left_disp'], 25,\n                   interpolation = 'midpoint')\n\n  Q3 = np.percentile(df['left_disp'], 75,\n                   interpolation = 'midpoint')\n  IQR = Q3 - Q1\n\n  up = Q3+1.5*IQR\n  low = Q1-1.5*IQR\n\n  df = df[df['left_disp']&lt;up]\n  df = df[df['left_disp']&gt;low]\n  return df\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef custom_multiplot(x, y_sets, title=None, xlabel=None, ylabel=None, legend_labels=None, legend_loc='best', grid=True, save_as=None, colors=None, title_size=16, label_font_size=12, tick_font_size=10, figsize=(10, 6), xlim=None, ylim=None, xlog=False, ylog=False, scatter=False):\n    \"\"\"\n    Create a customized plot with multiple y-axis parameters using Matplotlib.\n\n    Parameters:\n    - x: x-axis data (list or NumPy array)\n    - y_sets: List of y-axis data sets (list of lists or NumPy arrays)\n    - title: Plot title (string, optional)\n    - xlabel: Label for the x-axis (string, optional)\n    - ylabel: Label for the y-axis (string, optional)\n    - legend_labels: Labels for the legend (list of strings, optional)\n    - legend_loc: Location of the legend ('best', 'upper left', 'upper right', 'lower left', 'lower right', etc.)\n    - grid: Display grid lines (boolean, optional)\n    - save_as: File name to save the plot as an image (string, optional)\n    - colors: List of line colors (list of strings or tuples, optional)\n    - title_size: Font size for the plot title (int, optional)\n    - label_font_size: Font size for axis labels and legend (int, optional)\n    - tick_font_size: Font size for tick labels (int, optional)\n    - figsize: Figure size as a tuple (width, height) (optional)\n    - xlim: Tuple specifying the x-axis limits (e.g., (xmin, xmax)) (optional)\n    - ylim: Tuple specifying the y-axis limits (e.g., (ymin, ymax)) (optional)\n    - xlog: Enable logarithmic scaling for the x-axis (boolean, optional)\n    - ylog: Enable logarithmic scaling for the y-axis (boolean, optional)\n\n    Returns:\n    - None\n    \"\"\"\n    plt.figure(figsize=figsize)  # Adjust the figure size\n\n    if colors is None:\n        colors = ['blue', 'red', 'green', 'purple', 'orange', 'cyan', 'magenta']\n\n    for i, y in enumerate(y_sets):\n        color = colors[i % len(colors)]\n        label = legend_labels[i] if legend_labels and i &lt; len(legend_labels) else None\n\n        if scatter:\n            plt.scatter(x, y, label=label, color=color, s=30)\n        elif xlog:\n            plt.semilogx(x, y, label=label, color=color, linewidth=2)\n        elif ylog:\n            plt.semilogy(x, y, label=label, color=color, linewidth=2)\n        else:\n            plt.plot(x, y, label=label, color=color, linewidth=2)\n\n    if legend_labels:\n        plt.legend(legend_labels, loc=legend_loc, fontsize=label_font_size)\n\n    if title:\n        plt.title(title, fontsize=title_size)\n\n    if xlabel:\n        plt.xlabel(xlabel, fontsize=label_font_size)\n\n    if ylabel:\n        plt.ylabel(ylabel, fontsize=label_font_size)\n\n    if grid:\n        plt.grid(True)\n\n    if xlim:\n        plt.xlim(xlim)\n\n    if ylim:\n        plt.ylim(ylim)\n\n    plt.xticks(fontsize=tick_font_size)\n    plt.yticks(fontsize=tick_font_size)\n\n    if xlog:\n        plt.xscale('log')\n    if ylog:\n        plt.yscale('log')\n    ax = plt.gca()\n    #ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    #ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n    if save_as:\n        plt.savefig(save_as, dpi=300, bbox_inches='tight')\n\n    plt.show()\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support as score\nImport the required libraries\ndf = pd.read_csv('/content/drive/MyDrive/CVeSS/TB/Huckelberry/Feb_8/DE9C0013.csv')\n\ndf.set_axis(['sec', 'left_disp', 'right_disp', 'Yaw_disp', 'mag', 'x', 'y', 'z', 'temp', 'humid', 'sync', 'Tach', 'bat', 'date', 'UTC'], axis=\"columns\", inplace=True)\nThe provided dataset includes measured data, with a focus on the “sec,” “left_disp,” and “right_disp” columns, representing time, left sensor measurements, and right sensor measurements, respectively. The left and right sensors gauge the distance from the ground surface to a constant reference point, essentially capturing the surface figure of the track.\ncustom_multiplot(\n    df['sec'], [df['right_disp'], df['left_disp']],\n    title='Raw data',\n    xlabel='Time (s)',\n    ylabel='Distance',\n    legend_labels=['rigth', 'left'],\n    colors=['red', 'green'],\n    title_size=20,\n    label_font_size=14,\n    tick_font_size=12,\n    grid=True,\n    figsize=(20, 6),\n    xlog=False,\n    ylog=False\n)\n\nA funtion is defined to remove the outliers using IQR method\ndf = remove_outliers(df)\ndf.reset_index(inplace=True,drop=True)\ncustom_multiplot(\n    df['sec'], [df['right_disp'], df['left_disp']],\n    title='Filtered data',\n    xlabel='Time (s)',\n    ylabel='Distance',\n    legend_labels=['rigth', 'left'],\n    colors=['red', 'green'],\n    title_size=20,\n    label_font_size=14,\n    tick_font_size=12,\n    grid=True,\n    figsize=(20, 6),\n    xlog=False,\n    ylog=False\n)\n\nA small section of the data (from time 100 s to 150 s) is selected to be used for training\ndf_train = df[(df['sec']&lt;150) & (df['sec']&gt;100)]\n\ncustom_multiplot(\n    df_train['sec'], [df_train['right_disp'], df_train['left_disp']],\n    title='Raw data',\n    xlabel='Time (s)',\n    ylabel='Distance',\n    legend_labels=['rigth', 'left'],\n    colors=['red', 'green'],\n    title_size=20,\n    label_font_size=14,\n    tick_font_size=12,\n    grid=True,\n    figsize=(20, 6),\n    xlog=False,\n    ylog=False\n)\n\nBefore starting the training, we should manually label the data points related to ties and ballast. In the training dataframe, there are 55 ties. For each tie, start and end times are recorded in a dataframe called tb. We eill use this information to label all the data points.\ntb = pd.read_excel('/content/drive/MyDrive/CVeSS/TB/Huckelberry/Feb_8/tie_ballast.xlsx')\nLabeling all the data points take place here. In this block, we go through each data point and we check to see if it belongs to tb dataframe. If it was we have Tie (1), else, ballast (0).\nlabels = np.zeros(len(df_train))\n\nfor i in range(len(df_train)):\n  for j in range(len(tb['start'])):\n    if df_train.iloc[i]['sec']&lt;tb.iloc[j]['end']:\n      if df_train.iloc[i]['sec']&gt;tb.iloc[j]['start']:\n        labels[i] = 1\n\n\ndf_train['labels'] = labels\n\ncustom_multiplot(\n    df_train['sec'], [df_train['right_disp'], df_train['left_disp'], df_train['labels']],\n    title='Raw data',\n    xlabel='Time (s)',\n    ylabel='Distance',\n    legend_labels=['rigth', 'left', 'tie/ballast'],\n    colors=['red', 'green', 'blue'],\n    title_size=20,\n    label_font_size=14,\n    tick_font_size=12,\n    grid=True,\n    figsize=(20, 6),\n    xlog=False,\n    ylog=False\n)\n\nSave and reload the data for futur use\ndf_train.to_csv('/content/drive/MyDrive/CVeSS/TB/Huckelberry/Feb_8/Labeled_DE9C0013.csv')\ndf_train = pd.read_csv('/content/drive/MyDrive/CVeSS/TB/Huckelberry/Feb_8/Labeled_DE9C0013.csv')\n\n\nAll the measured data will be stored in sequences of certain size. So the machine learning model will look at sequence of data not a single data point.\n\n\n\nThree inputs are defined here to investigate which one yields better results.\n\nRaw measurment\nDifference of right and left\nStandard deviation of difference\n\nws = 50\n\nRDbatch = []\nLDbatch = []\nDiffbatch = []\nSTDDiff = []\ntarget = []\n\n#len(df_train) // ws * ws\nfor i in range(ws, len(df_train)):\n  RDbatch.append(df_train.iloc[i-ws:i]['right_disp'])\n  LDbatch.append(df_train.iloc[i-ws:i]['left_disp'])\n  Diffbatch.append(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp'])\n  STDDiff.append(np.std(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp']))\n  if sum(df_train.iloc[i-ws:i]['labels']) &gt;= ws/2:\n    target.append(1)\n  else:\n    target.append(0)\nSTDofDiff = np.zeros([len(STDDiff), ws])\n\nfor i in range(ws, len(STDofDiff)):\n  STDofDiff[i-ws][:] = np.array([STDDiff[i-ws:i]])\nData is scaled using StandardScaler\n\n\nX_train, X_test, y_train, y_test = train_test_split(np.array(LDbatch), target, test_size=0.25, random_state=0)\n\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)\n\ndecision_tree = DecisionTreeClassifier(random_state=456)\ndecision_tree.fit(X_train, y_train)\n\ny_pred = decision_tree.predict(X_test)\nprecision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\nprint(\"F Score for using one measurement : \", fscore)\nF Score for using one measurement : 0.959993349532803\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(np.array(Diffbatch), target, test_size=0.25, random_state=0)\n\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)\n\ndecision_tree = DecisionTreeClassifier(random_state=456)\ndecision_tree.fit(X_train, y_train)\n\ny_pred = decision_tree.predict(X_test)\nprecision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\nprint(\"F Score for using difference of measurments : \", fscore)\nF Score for using difference of measurments : 0.9173980177749999\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(np.array(STDofDiff), target, test_size=0.25, random_state=0)\n\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)\n\ndecision_tree = DecisionTreeClassifier(random_state=456)\ndecision_tree.fit(X_train, y_train)\n\ny_pred = decision_tree.predict(X_test)\nprecision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\nprint(\"F Score for using standard deviation of difference of measurmentst : \", fscore)\nF Score for using standard deviation of difference of measurmentst : 0.9399797522284663\nIt can be seen that the raw measurments resulted in a better F-score. However, STD of difference more robust and is less sensitive in tie elevation. So we decided to move on with STD of difference as input.\n\n\n\n\nIn this section the optimum window size will be selected\nt = []\nfscoreVal = []\nmodel = []\n\nfor w in range(50, 251, 50):\n  ws = w\n  RDbatch = []\n  LDbatch = []\n  Diffbatch = []\n  STDDiff = []\n  target = []\n\n  #len(df_train) // ws * ws\n  for i in range(ws, len(df_train)):\n    RDbatch.append(df_train.iloc[i-ws:i]['right_disp'])\n    LDbatch.append(df_train.iloc[i-ws:i]['left_disp'])\n    Diffbatch.append(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp'])\n    STDDiff.append(np.std(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp']))\n    if sum(df_train.iloc[i-ws:i]['labels']) &gt;= ws*0.75:\n      target.append(1)\n    else:\n      target.append(0)\n\n  STDofDiff = np.zeros([len(STDDiff), ws])\n  for i in range(ws, len(STDofDiff)):\n    STDofDiff[i-ws][:] = np.array([STDDiff[i-ws:i]])\n\n\n  X_train, X_test, y_train, y_test = train_test_split(STDofDiff, target, test_size=0.25, random_state=0)\n  scaler = StandardScaler()\n  X_train=scaler.fit_transform(X_train)\n  X_test=scaler.fit_transform(X_test)\n  decision_tree = DecisionTreeClassifier(random_state=456)\n  decision_tree.fit(X_train, y_train)\n  y_pred = decision_tree.predict(X_test)\n  precision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\n  model.append(decision_tree)\n  t.append(ws)\n  fscoreVal.append(fscore)\ncustom_multiplot(\n    t, [fscoreVal],\n    title='Logarithmic Scale Plot',\n    xlabel='X-axis (log scale)',\n    ylabel='Y-axis (log scale)',\n    legend_labels=['y1', 'y2'],\n    colors=['blue', 'red'],\n    title_size=20,\n    label_font_size=14,\n    tick_font_size=12,\n    grid=True,\n    save_as='logarithmic_plot.png',\n    figsize=(10, 6),\n    xlog=False,\n    ylog=False\n)"
  },
  {
    "objectID": "index.html#segmentizing-the-data",
    "href": "index.html#segmentizing-the-data",
    "title": "MortezaML",
    "section": "",
    "text": "All the measured data will be stored in sequences of certain size. So the machine learning model will look at sequence of data not a single data point."
  },
  {
    "objectID": "index.html#inputs",
    "href": "index.html#inputs",
    "title": "MortezaML",
    "section": "",
    "text": "Three inputs are defined here to investigate which one yields better results.\n\nRaw measurment\nDifference of right and left\nStandard deviation of difference\n\nws = 50\n\nRDbatch = []\nLDbatch = []\nDiffbatch = []\nSTDDiff = []\ntarget = []\n\n#len(df_train) // ws * ws\nfor i in range(ws, len(df_train)):\n  RDbatch.append(df_train.iloc[i-ws:i]['right_disp'])\n  LDbatch.append(df_train.iloc[i-ws:i]['left_disp'])\n  Diffbatch.append(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp'])\n  STDDiff.append(np.std(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp']))\n  if sum(df_train.iloc[i-ws:i]['labels']) &gt;= ws/2:\n    target.append(1)\n  else:\n    target.append(0)\nSTDofDiff = np.zeros([len(STDDiff), ws])\n\nfor i in range(ws, len(STDofDiff)):\n  STDofDiff[i-ws][:] = np.array([STDDiff[i-ws:i]])\nData is scaled using StandardScaler\n\n\nX_train, X_test, y_train, y_test = train_test_split(np.array(LDbatch), target, test_size=0.25, random_state=0)\n\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)\n\ndecision_tree = DecisionTreeClassifier(random_state=456)\ndecision_tree.fit(X_train, y_train)\n\ny_pred = decision_tree.predict(X_test)\nprecision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\nprint(\"F Score for using one measurement : \", fscore)\nF Score for using one measurement : 0.959993349532803\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(np.array(Diffbatch), target, test_size=0.25, random_state=0)\n\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)\n\ndecision_tree = DecisionTreeClassifier(random_state=456)\ndecision_tree.fit(X_train, y_train)\n\ny_pred = decision_tree.predict(X_test)\nprecision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\nprint(\"F Score for using difference of measurments : \", fscore)\nF Score for using difference of measurments : 0.9173980177749999\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(np.array(STDofDiff), target, test_size=0.25, random_state=0)\n\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)\n\ndecision_tree = DecisionTreeClassifier(random_state=456)\ndecision_tree.fit(X_train, y_train)\n\ny_pred = decision_tree.predict(X_test)\nprecision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\nprint(\"F Score for using standard deviation of difference of measurmentst : \", fscore)\nF Score for using standard deviation of difference of measurmentst : 0.9399797522284663\nIt can be seen that the raw measurments resulted in a better F-score. However, STD of difference more robust and is less sensitive in tie elevation. So we decided to move on with STD of difference as input."
  },
  {
    "objectID": "index.html#optimum-window-size",
    "href": "index.html#optimum-window-size",
    "title": "MortezaML",
    "section": "",
    "text": "In this section the optimum window size will be selected\nt = []\nfscoreVal = []\nmodel = []\n\nfor w in range(50, 251, 50):\n  ws = w\n  RDbatch = []\n  LDbatch = []\n  Diffbatch = []\n  STDDiff = []\n  target = []\n\n  #len(df_train) // ws * ws\n  for i in range(ws, len(df_train)):\n    RDbatch.append(df_train.iloc[i-ws:i]['right_disp'])\n    LDbatch.append(df_train.iloc[i-ws:i]['left_disp'])\n    Diffbatch.append(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp'])\n    STDDiff.append(np.std(df_train.iloc[i-ws:i]['left_disp'] - df_train.iloc[i-ws:i]['right_disp']))\n    if sum(df_train.iloc[i-ws:i]['labels']) &gt;= ws*0.75:\n      target.append(1)\n    else:\n      target.append(0)\n\n  STDofDiff = np.zeros([len(STDDiff), ws])\n  for i in range(ws, len(STDofDiff)):\n    STDofDiff[i-ws][:] = np.array([STDDiff[i-ws:i]])\n\n\n  X_train, X_test, y_train, y_test = train_test_split(STDofDiff, target, test_size=0.25, random_state=0)\n  scaler = StandardScaler()\n  X_train=scaler.fit_transform(X_train)\n  X_test=scaler.fit_transform(X_test)\n  decision_tree = DecisionTreeClassifier(random_state=456)\n  decision_tree.fit(X_train, y_train)\n  y_pred = decision_tree.predict(X_test)\n  precision,recall,fscore,support=score(y_test,y_pred,average='macro')\n\n  model.append(decision_tree)\n  t.append(ws)\n  fscoreVal.append(fscore)\ncustom_multiplot(\n    t, [fscoreVal],\n    title='Logarithmic Scale Plot',\n    xlabel='X-axis (log scale)',\n    ylabel='Y-axis (log scale)',\n    legend_labels=['y1', 'y2'],\n    colors=['blue', 'red'],\n    title_size=20,\n    label_font_size=14,\n    tick_font_size=12,\n    grid=True,\n    save_as='logarithmic_plot.png',\n    figsize=(10, 6),\n    xlog=False,\n    ylog=False\n)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  }
]
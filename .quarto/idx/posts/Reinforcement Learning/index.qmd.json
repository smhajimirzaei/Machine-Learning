{"title":"Reinforcement Learning","markdown":{"yaml":{"title":"Reinforcement Learning","author":"Morteza Mirzaei","date":"2023-12-09","categories":["code","analysis"],"image":"image.jpg"},"headingText":"RL for Drone Navigation","containsRefs":false,"markdown":"\n\n\nIn this project, an RL environment will be setup to train an object to start from a starting point and pan towards the destination in 2D space\n\n``` python\n!pip install stable_baselines3\n!pip install gymnasium\n!pip install tensorboard\n!pip install ipywidgets\n```\n\n``` python\nfrom google.colab import drive\ndrive.mount('/content/drive')\n```\n\n``` python\nimport stable_baselines3\nfrom gymnasium import spaces, Env\n\nimport numpy as np\n\nGRID_SIZE  = np.array([10.0,    10.0], dtype=np.float32)\nMAGN_RANGE = np.array([0.0,      1.0], dtype=np.float32)\nHEAD_RANGE = np.array([0.0,2 * np.pi], dtype=np.float32)\n\nd_magn = 0.1\n\nTARGET_POSITION      = np.array([7.0, 5.0], dtype=np.float32)\nREWARD_CIRCLE_RADIUS = 0.5 #units\nMAX_EPISODE_STEPS    = 512\nMAX_RW_CIRCLE_STEPS  = 16\n\nclass Env2D(Env):\n\n    def __init__(self):\n\n        # Action space: Magnitude and Heading\n        self.action_space = spaces.Box(low  = np.array([MAGN_RANGE[0], HEAD_RANGE[0]]),\n                                       high = np.array([MAGN_RANGE[1], HEAD_RANGE[1]]),\n                                       dtype=np.float32)\n\n        # Observation space: Position, Target Position, Heading\n\n        low_obs  = np.array([#Minimum values these can take\n                                              0,                  0,\n                             TARGET_POSITION[0], TARGET_POSITION[1],\n                             HEAD_RANGE[0],\n                             MAGN_RANGE[0]\n        ], dtype=np.float32)\n\n        high_obs = np.array([#Maximum values these can take\n                             GRID_SIZE[0]      ,        GRID_SIZE[1],\n                             TARGET_POSITION[0],  TARGET_POSITION[1],\n                             HEAD_RANGE[1],\n                             MAGN_RANGE[1]\n        ], dtype=np.float32)\n\n        self.observation_space = spaces.Box(low = low_obs, high = high_obs, dtype = np.float32)\n\n        # Call reset to initialize the agent's position and heading\n        self.reset()\n\n    def reset(self, seed=None):\n\n        if seed:\n            np.random.seed(seed)\n\n        # Initialize agent positioned randomly within the grid, with random heading and 0 magnitude\n        self.position  = np.array([np.random.uniform(0, GRID_SIZE[0]), np.random.uniform(0, GRID_SIZE[1])], dtype=np.float32)\n        self.heading   = np.random.uniform(HEAD_RANGE[0], HEAD_RANGE[1])\n        self.magnitude = 0.0\n\n        # Initialize reward buffer for the last MAX_RW_CIRCLE steps\n        self.reward_buffer = np.full((MAX_RW_CIRCLE_STEPS,), -np.inf)\n\n        #Episode length counter\n        self.current_step = 0\n\n        return self._get_obs(), {}\n\n    def _get_obs(self):\n        # Return observation space\n        return np.array([\n            self.position[0]  , self.position[1],\n            TARGET_POSITION[0], TARGET_POSITION[1],\n            self.heading,\n            self.magnitude\n        ], dtype=np.float32)\n\n    def step(self, action):\n\n        # Update and clip heading and magnitude based on action\n        self.magnitude = np.clip(action[0], MAGN_RANGE[0], MAGN_RANGE[1])\n        self.heading = (self.heading + action[1]) % (2 * np.pi)\n\n        # Move agent according to given heading\n        dx = self.magnitude * np.cos(self.heading) * d_magn\n        dy = self.magnitude * np.sin(self.heading) * d_magn\n        self.position = np.array([self.position[0] + dx, self.position[1] + dy], dtype=np.float32)\n        self.position = np.clip(self.position, [0, 0], GRID_SIZE)\n\n        # Calculate reward\n        distance_to_target = np.linalg.norm(self.position - TARGET_POSITION)\n        reward = -distance_to_target\n\n        # Update reward buffer\n        self.reward_buffer[:-1] = self.reward_buffer[1:]\n        self.reward_buffer[-1] = reward\n\n        #increment step counter\n        self.current_step += 1\n\n        # Calculate 'truncated'\n        truncated = self.current_step >= MAX_EPISODE_STEPS\n\n        # Calculate 'terminated'\n        terminated = np.all(self.reward_buffer >= -REWARD_CIRCLE_RADIUS)\n\n        return self._get_obs(), reward, bool(terminated), bool(truncated), {}\n\n    def render(self, mode='human'):\n        pass\n\n    def close(self):\n        pass\n```\n\nIn order to make it faster it was trained for 2500000 steps but actually it needs 25000000 steps to successfully reach the target\n\n``` python\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_checker import check_env\n\nenv = Env2D()\ncheck_env(env)\n\n#log\nlog_dir = \"logs/\" #tensorboard --logdir logs\n\n# Initialize the agent\ntrain = True #true\nif train:\n    model = PPO(\"MlpPolicy\", env, verbose=0, tensorboard_log=log_dir)\n    model.learn(total_timesteps=2500000, progress_bar=True)\n```\n\n``` python\nenv = Env2D()\ncheck_env(env)\n\nframes_all = []\nfor _ in range(10):\n\n    #Run one collect\n    obs, _info = env.reset()\n    frames = []\n\n    # Run an episode\n    terminated = truncated = False\n    while not (terminated or truncated):\n\n        #compute action\n        action, state_ = model.predict(obs, deterministic=True)\n\n        #store obs-state-action\n        frames.append((obs, action))\n\n        #environment takes action\n        obs, reward, terminated, truncated, _info = env.step(action)\n\n    frames_all.append(frames)\n\nenv.close()\n```\n\n![](images/13.png)\n\nFor each iteration, the motion of object can be visualized\n\n``` python\nimport pygame\nimport math\n\n# Initialize pygame\npygame.init()\n\n# Set the dimensions of the window\nWIDTH, HEIGHT = 800, 800\nwin = pygame.display.set_mode((WIDTH, HEIGHT))\npygame.display.set_caption(\"Simple Animation\")\n\n# Colors\nWHITE = (255, 255, 255)\nRED = (255, 0, 0)\nBLUE = (0, 0, 255)\nGREEN = (0, 255, 0)\n\n# Scale factor to fit (10,10) position within the window size\nSCALE = 80 # 800/10 = 80\n\ndef draw_position_and_heading(position, target, heading, thrust):\n\n    # Draw the position\n    pygame.draw.circle(win, RED, (int(position[0]*SCALE), int(position[1]*SCALE)), 10)\n\n    # Draw the target\n    pygame.draw.circle(win, GREEN, (int(target[0]*SCALE), int(target[1]*SCALE)), 5)\n\n    # Draw the heading\n    end_x = int(position[0]*SCALE + thrust * 50 * math.cos(heading))\n    end_y = int(position[1]*SCALE + thrust * 50 * math.sin(heading))\n    pygame.draw.line(win, BLUE, (int(position[0]*SCALE), int(position[1]*SCALE)), (end_x, end_y), 2)\n\ndef main(frames):\n    clock = pygame.time.Clock()\n    running = True\n    frames_idx = 0\n\n    while running:\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                running = False\n\n        win.fill(WHITE)  # Fill the screen with white\n        for frames_set in frames_all:\n            if frames_idx < len(frames_set):\n                frame = frames_set[frames_idx]\n                position = frame[0][0:2]\n                target = frame[0][2:4]\n                heading = frame[0][4]\n                thrust = frame[1][0]\n                draw_position_and_heading(position, target, heading, thrust)\n\n        frames_idx += 1\n        pygame.time.wait(100)\n        pygame.display.update()\n\n    pygame.quit()\n\nmain(frames_all)\n```\n","srcMarkdownNoYaml":"\n\n# RL for Drone Navigation\n\nIn this project, an RL environment will be setup to train an object to start from a starting point and pan towards the destination in 2D space\n\n``` python\n!pip install stable_baselines3\n!pip install gymnasium\n!pip install tensorboard\n!pip install ipywidgets\n```\n\n``` python\nfrom google.colab import drive\ndrive.mount('/content/drive')\n```\n\n``` python\nimport stable_baselines3\nfrom gymnasium import spaces, Env\n\nimport numpy as np\n\nGRID_SIZE  = np.array([10.0,    10.0], dtype=np.float32)\nMAGN_RANGE = np.array([0.0,      1.0], dtype=np.float32)\nHEAD_RANGE = np.array([0.0,2 * np.pi], dtype=np.float32)\n\nd_magn = 0.1\n\nTARGET_POSITION      = np.array([7.0, 5.0], dtype=np.float32)\nREWARD_CIRCLE_RADIUS = 0.5 #units\nMAX_EPISODE_STEPS    = 512\nMAX_RW_CIRCLE_STEPS  = 16\n\nclass Env2D(Env):\n\n    def __init__(self):\n\n        # Action space: Magnitude and Heading\n        self.action_space = spaces.Box(low  = np.array([MAGN_RANGE[0], HEAD_RANGE[0]]),\n                                       high = np.array([MAGN_RANGE[1], HEAD_RANGE[1]]),\n                                       dtype=np.float32)\n\n        # Observation space: Position, Target Position, Heading\n\n        low_obs  = np.array([#Minimum values these can take\n                                              0,                  0,\n                             TARGET_POSITION[0], TARGET_POSITION[1],\n                             HEAD_RANGE[0],\n                             MAGN_RANGE[0]\n        ], dtype=np.float32)\n\n        high_obs = np.array([#Maximum values these can take\n                             GRID_SIZE[0]      ,        GRID_SIZE[1],\n                             TARGET_POSITION[0],  TARGET_POSITION[1],\n                             HEAD_RANGE[1],\n                             MAGN_RANGE[1]\n        ], dtype=np.float32)\n\n        self.observation_space = spaces.Box(low = low_obs, high = high_obs, dtype = np.float32)\n\n        # Call reset to initialize the agent's position and heading\n        self.reset()\n\n    def reset(self, seed=None):\n\n        if seed:\n            np.random.seed(seed)\n\n        # Initialize agent positioned randomly within the grid, with random heading and 0 magnitude\n        self.position  = np.array([np.random.uniform(0, GRID_SIZE[0]), np.random.uniform(0, GRID_SIZE[1])], dtype=np.float32)\n        self.heading   = np.random.uniform(HEAD_RANGE[0], HEAD_RANGE[1])\n        self.magnitude = 0.0\n\n        # Initialize reward buffer for the last MAX_RW_CIRCLE steps\n        self.reward_buffer = np.full((MAX_RW_CIRCLE_STEPS,), -np.inf)\n\n        #Episode length counter\n        self.current_step = 0\n\n        return self._get_obs(), {}\n\n    def _get_obs(self):\n        # Return observation space\n        return np.array([\n            self.position[0]  , self.position[1],\n            TARGET_POSITION[0], TARGET_POSITION[1],\n            self.heading,\n            self.magnitude\n        ], dtype=np.float32)\n\n    def step(self, action):\n\n        # Update and clip heading and magnitude based on action\n        self.magnitude = np.clip(action[0], MAGN_RANGE[0], MAGN_RANGE[1])\n        self.heading = (self.heading + action[1]) % (2 * np.pi)\n\n        # Move agent according to given heading\n        dx = self.magnitude * np.cos(self.heading) * d_magn\n        dy = self.magnitude * np.sin(self.heading) * d_magn\n        self.position = np.array([self.position[0] + dx, self.position[1] + dy], dtype=np.float32)\n        self.position = np.clip(self.position, [0, 0], GRID_SIZE)\n\n        # Calculate reward\n        distance_to_target = np.linalg.norm(self.position - TARGET_POSITION)\n        reward = -distance_to_target\n\n        # Update reward buffer\n        self.reward_buffer[:-1] = self.reward_buffer[1:]\n        self.reward_buffer[-1] = reward\n\n        #increment step counter\n        self.current_step += 1\n\n        # Calculate 'truncated'\n        truncated = self.current_step >= MAX_EPISODE_STEPS\n\n        # Calculate 'terminated'\n        terminated = np.all(self.reward_buffer >= -REWARD_CIRCLE_RADIUS)\n\n        return self._get_obs(), reward, bool(terminated), bool(truncated), {}\n\n    def render(self, mode='human'):\n        pass\n\n    def close(self):\n        pass\n```\n\nIn order to make it faster it was trained for 2500000 steps but actually it needs 25000000 steps to successfully reach the target\n\n``` python\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_checker import check_env\n\nenv = Env2D()\ncheck_env(env)\n\n#log\nlog_dir = \"logs/\" #tensorboard --logdir logs\n\n# Initialize the agent\ntrain = True #true\nif train:\n    model = PPO(\"MlpPolicy\", env, verbose=0, tensorboard_log=log_dir)\n    model.learn(total_timesteps=2500000, progress_bar=True)\n```\n\n``` python\nenv = Env2D()\ncheck_env(env)\n\nframes_all = []\nfor _ in range(10):\n\n    #Run one collect\n    obs, _info = env.reset()\n    frames = []\n\n    # Run an episode\n    terminated = truncated = False\n    while not (terminated or truncated):\n\n        #compute action\n        action, state_ = model.predict(obs, deterministic=True)\n\n        #store obs-state-action\n        frames.append((obs, action))\n\n        #environment takes action\n        obs, reward, terminated, truncated, _info = env.step(action)\n\n    frames_all.append(frames)\n\nenv.close()\n```\n\n![](images/13.png)\n\nFor each iteration, the motion of object can be visualized\n\n``` python\nimport pygame\nimport math\n\n# Initialize pygame\npygame.init()\n\n# Set the dimensions of the window\nWIDTH, HEIGHT = 800, 800\nwin = pygame.display.set_mode((WIDTH, HEIGHT))\npygame.display.set_caption(\"Simple Animation\")\n\n# Colors\nWHITE = (255, 255, 255)\nRED = (255, 0, 0)\nBLUE = (0, 0, 255)\nGREEN = (0, 255, 0)\n\n# Scale factor to fit (10,10) position within the window size\nSCALE = 80 # 800/10 = 80\n\ndef draw_position_and_heading(position, target, heading, thrust):\n\n    # Draw the position\n    pygame.draw.circle(win, RED, (int(position[0]*SCALE), int(position[1]*SCALE)), 10)\n\n    # Draw the target\n    pygame.draw.circle(win, GREEN, (int(target[0]*SCALE), int(target[1]*SCALE)), 5)\n\n    # Draw the heading\n    end_x = int(position[0]*SCALE + thrust * 50 * math.cos(heading))\n    end_y = int(position[1]*SCALE + thrust * 50 * math.sin(heading))\n    pygame.draw.line(win, BLUE, (int(position[0]*SCALE), int(position[1]*SCALE)), (end_x, end_y), 2)\n\ndef main(frames):\n    clock = pygame.time.Clock()\n    running = True\n    frames_idx = 0\n\n    while running:\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                running = False\n\n        win.fill(WHITE)  # Fill the screen with white\n        for frames_set in frames_all:\n            if frames_idx < len(frames_set):\n                frame = frames_set[frames_idx]\n                position = frame[0][0:2]\n                target = frame[0][2:4]\n                heading = frame[0][4]\n                thrust = frame[1][0]\n                draw_position_and_heading(position, target, heading, thrust)\n\n        frames_idx += 1\n        pygame.time.wait(100)\n        pygame.display.update()\n\n    pygame.quit()\n\nmain(frames_all)\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Reinforcement Learning","author":"Morteza Mirzaei","date":"2023-12-09","categories":["code","analysis"],"image":"image.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}